{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LsgIX5RV7Pp",
        "outputId": "9d3bf86d-09d2-478e-920e-1d48c99e5b93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1cPYejv5JKf"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fol2AgnMJv4t",
        "outputId": "24d3241a-3e44-4807-8708-c021237e0dc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.12.2)\n",
            "Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, uvicorn\n",
            "Successfully installed h11-0.14.0 uvicorn-0.32.0\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.2-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting starlette<0.41.0,>=0.37.2 (from fastapi)\n",
            "  Downloading starlette-0.40.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.41.0,>=0.37.2->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.41.0,>=0.37.2->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.41.0,>=0.37.2->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.41.0,>=0.37.2->fastapi) (1.2.2)\n",
            "Downloading fastapi-0.115.2-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.40.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.115.2 starlette-0.40.0\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Downloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.12\n",
            "Collecting simple_lama_inpainting\n",
            "  Downloading simple_lama_inpainting-0.1.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting fire<0.6.0,>=0.5.0 (from simple_lama_inpainting)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from simple_lama_inpainting) (1.26.4)\n",
            "Requirement already satisfied: opencv-python<5.0.0.0,>=4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from simple_lama_inpainting) (4.10.0.84)\n",
            "Collecting pillow<10.0.0,>=9.5.0 (from simple_lama_inpainting)\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: torch!=2.0.1,>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from simple_lama_inpainting) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from simple_lama_inpainting) (0.19.1+cu121)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire<0.6.0,>=0.5.0->simple_lama_inpainting) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire<0.6.0,>=0.5.0->simple_lama_inpainting) (2.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple_lama_inpainting) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple_lama_inpainting) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple_lama_inpainting) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple_lama_inpainting) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple_lama_inpainting) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple_lama_inpainting) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0.1,>=1.13.1->simple_lama_inpainting) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=2.0.1,>=1.13.1->simple_lama_inpainting) (1.3.0)\n",
            "Downloading simple_lama_inpainting-0.1.2-py3-none-any.whl (9.6 kB)\n",
            "Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=cef8f449d519e235b5863be81ce3095a3d74e6478304264d3b42c745732548b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built fire\n",
            "Installing collected packages: pillow, fire, simple_lama_inpainting\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 10.4.0\n",
            "    Uninstalling pillow-10.4.0:\n",
            "      Successfully uninstalled pillow-10.4.0\n",
            "Successfully installed fire-0.5.0 pillow-9.5.0 simple_lama_inpainting-0.1.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "a728253795f84afebf2b38153b67350d",
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting diffusers\n",
            "  Downloading diffusers-0.30.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.24.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.5.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.8.30)\n",
            "Downloading diffusers-0.30.3-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diffusers\n",
            "Successfully installed diffusers-0.30.3\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (0.0.12)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.5.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.4)\n",
            "Collecting futures\n",
            "  Downloading futures-3.0.5.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: futures\n",
            "  Building wheel for futures (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for futures: filename=futures-3.0.5-py3-none-any.whl size=14069 sha256=3c83c8fe21ff8df240865b62f51123470c10f1e285aae632e3b136433e0f3fa4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/af/93/48739d464ba97d4cdc77c627d282f9794c8d276e42aaa92160\n",
            "Successfully built futures\n",
            "Installing collected packages: futures\n",
            "Successfully installed futures-3.0.5\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "e320b9f9b0db4cacb707c5a2097b94a0",
              "pip_warning": {
                "packages": [
                  "concurrent"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install uvicorn\n",
        "!pip install fastapi\n",
        "!pip install python-multipart\n",
        "!pip install simple_lama_inpainting\n",
        "!pip install diffusers\n",
        "!pip install python-multipart opencv-python-headless pillow\n",
        "!pip install futures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YweW6VMrU4aC",
        "outputId": "58f82537-045f-43d8-b62a-02b4ab2b1154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'deepfillv2_colab'...\n",
            "remote: Enumerating objects: 99, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 99 (delta 2), reused 1 (delta 1), pack-reused 96 (from 1)\u001b[K\n",
            "Receiving objects: 100% (99/99), 571.56 KiB | 19.71 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n",
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1uMghKl883-9hDLhSiI8lRbHCzCmmRwV-&export=download\n",
            "To: /content/deepfillv2_WGAN_G_epoch40_batchsize4.pth\n",
            "100% 64.8M/64.8M [00:00<00:00, 68.1MB/s]\n",
            "/content\n",
            "Cloning into 'GroundingDINO'...\n",
            "remote: Enumerating objects: 463, done.\u001b[K\n",
            "remote: Counting objects: 100% (240/240), done.\u001b[K\n",
            "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
            "remote: Total 463 (delta 175), reused 136 (delta 136), pack-reused 223 (from 1)\u001b[K\n",
            "Receiving objects: 100% (463/463), 12.87 MiB | 9.48 MiB/s, done.\n",
            "Resolving deltas: 100% (241/241), done.\n",
            "/content/GroundingDINO\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h/content/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py ; exist: True\n",
            "/content/weights\n",
            "/content/weights/groundingdino_swint_ogc.pth ; exist: True\n",
            "/content/GroundingDINO\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# DeepFillv2 セットアップ\n",
        "HOME = \"/content\"\n",
        "%cd {HOME}\n",
        "!git clone https://github.com/vrindaprabhu/deepfillv2_colab.git\n",
        "!gdown \"https://drive.google.com/u/0/uc?id=1uMghKl883-9hDLhSiI8lRbHCzCmmRwV-&export=download\"\n",
        "!mv /content/deepfillv2_WGAN_G_epoch40_batchsize4.pth deepfillv2_colab/model/deepfillv2_WGAN.pth\n",
        "\n",
        "# GroundingDINO セットアップ\n",
        "%cd {HOME}\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd {HOME}/GroundingDINO\n",
        "!pip install -q -e .\n",
        "!pip install -q roboflow\n",
        "\n",
        "# GroundingDINOのコンフィグとウェイトのダウンロード\n",
        "CONFIG_PATH = os.path.join(HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
        "print(CONFIG_PATH, \"; exist:\", os.path.isfile(CONFIG_PATH))\n",
        "\n",
        "!mkdir -p {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
        "WEIGHTS_NAME = \"groundingdino_swint_ogc.pth\"\n",
        "WEIGHTS_PATH = os.path.join(HOME, \"weights\", WEIGHTS_NAME)\n",
        "print(WEIGHTS_PATH, \"; exist:\", os.path.isfile(WEIGHTS_PATH))\n",
        "%cd {HOME}/GroundingDINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJBuwyUyVBG2",
        "outputId": "02478192-b8bb-4491-a3cb-79bcc26ff7a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import supervision as sv\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pycocotools.mask as mask_util\n",
        "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
        "from fastapi import FastAPI, File, UploadFile, Form\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import FileResponse\n",
        "from fastapi.responses import HTMLResponse\n",
        "import shutil\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "from diffusers import StableDiffusionInpaintPipeline\n",
        "import torch\n",
        "from PIL import Image\n",
        "from simple_lama_inpainting import SimpleLama\n",
        "import numpy as np\n",
        "from groundingdino.util.inference import load_model, load_image, predict\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MiniBatchKMeans, Birch, SpectralClustering, MeanShift, OPTICS\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import threading\n",
        "from fastapi import FastAPI, File, UploadFile, Form\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import FileResponse\n",
        "import shutil\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "from diffusers import StableDiffusionInpaintPipeline\n",
        "import torch\n",
        "from PIL import Image\n",
        "from simple_lama_inpainting import SimpleLama\n",
        "import numpy as np\n",
        "from groundingdino.util.inference import load_model, load_image, predict\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MiniBatchKMeans, Birch, SpectralClustering, MeanShift, OPTICS\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import concurrent.futures  # 追加\n",
        "from pathlib import Path\n",
        "import os\n",
        "import concurrent.futures\n",
        "from typing import Tuple\n",
        "from types import SimpleNamespace\n",
        "import subprocess\n",
        "import shutil\n",
        "import os\n",
        "HOME = \"/content\"\n",
        "CONFIG_PATH = os.path.join(HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
        "WEIGHTS_NAME = \"groundingdino_swint_ogc.pth\"\n",
        "WEIGHTS_PATH = os.path.join(HOME, \"weights\", WEIGHTS_NAME)\n",
        "detection_model = load_model(CONFIG_PATH, WEIGHTS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w6jFnKuI5Uz",
        "outputId": "9c79a5ba-2f52-4db9-9957-13ab2049ac48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "# 認証トークンをファイルから読み込み\n",
        "with open('/content/drive/MyDrive/ngrok_token.txt', 'r') as file:\n",
        "    ngrok_auth_token = file.read().strip()\n",
        "\n",
        "# 認証トークンを使用して ngrok に設定\n",
        "!ngrok authtoken {ngrok_auth_token}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yR7ekMMDRvdY"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageFilter\n",
        "from concurrent.futures import ThreadPoolExecutor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRN2t_9gMQWx"
      },
      "source": [
        "ここからが本番コード\n",
        "上は、セットアップコード\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDgOItGv5q-n",
        "outputId": "6268d0c9-d26e-41d2-b16f-0eb2d2eeeea5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [10005]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.63905585 0.         0.         0.73494852 0.90994442 0.74501801\n",
            " 0.72196484 0.33889359 0.71385443 0.80644011 0.         0.70761728\n",
            " 0.71582139 0.7175144  0.         5.18829823 0.         0.74006587\n",
            " 1.14363098 0.85681629 1.66955853]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.44100689501895424\n",
            "0.44100689501895424\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.63905585 0.         0.         0.73494852 0.90994442 0.74501801\n",
            " 0.72196484 0.33889359 0.71385443 0.80644011 0.         0.70761728\n",
            " 0.71582139 0.7175144  0.         5.18829823 0.         0.74006587\n",
            " 1.14363098 0.85681629 1.66955853]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.53678364 1.04500699 0.84846926 1.09008288 1.18172848 1.01549697\n",
            " 1.11349988 0.64324737 0.74980772 0.84854829 0.66166866 0.86639017\n",
            " 0.87481177 1.12040401 0.46095031 0.66603762 0.53275341 0.55364895\n",
            " 1.15581501 0.74789608 0.48301297]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.91851437 1.30900049 1.37864554 1.26931775 1.39563179 0.9400456\n",
            " 1.49151349 0.93952429 0.85543627 0.82348752 0.93703705 0.83409297\n",
            " 1.01317775 0.98500532 0.95748758 1.33289671 0.47742242 0.86867607\n",
            " 1.07717526 0.70875305 1.16394401]\n",
            "INFO:     210.156.37.4:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.44100689501895424\n",
            "0.44100689501895424\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.91851437 1.30900049 1.37864554 1.26931775 1.39563179 0.9400456\n",
            " 1.49151349 0.93952429 0.85543627 0.82348752 0.93703705 0.83409297\n",
            " 1.01317775 0.98500532 0.95748758 1.33289671 0.47742242 0.86867607\n",
            " 1.07717526 0.70875305 1.16394401]\n",
            "INFO:     210.156.37.4:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.44100689501895424\n",
            "lama\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "final text_encoder_type: bert-base-uncased\n",
            "[1.84353757 1.49752593 1.14732158 0.9631691  1.06250882 1.45621419\n",
            " 0.43586376 1.2385515  0.96529663 0.97457856 0.43063128 1.00473404\n",
            " 0.90380496 0.93929005 0.89797676 0.96337062 0.42601952 1.092592\n",
            " 1.018121   0.62337577 0.43020326]\n",
            "INFO:     210.156.37.4:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.44100689501895424\n",
            "0.44100689501895424\n",
            "0.44100689501895424\n",
            "0.44100689501895424\n",
            "0.44100689501895424\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 406, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 93, in __call__\n",
            "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 144, in simple_response\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 715, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 735, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 74, in app\n",
            "    await response(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 348, in __call__\n",
            "    await self._handle_simple(send, send_header_only)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 377, in _handle_simple\n",
            "    await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": more_body})\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 48, in sender\n",
            "    await send(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 48, in sender\n",
            "    await send(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 148, in send\n",
            "    await send(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 162, in _send\n",
            "    await send(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 510, in send\n",
            "    output = self.conn.send(event=h11.EndOfMessage())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/h11/_connection.py\", line 512, in send\n",
            "    data_list = self.send_with_data_passthrough(event)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/h11/_connection.py\", line 545, in send_with_data_passthrough\n",
            "    writer(event, data_list.append)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/h11/_writers.py\", line 67, in __call__\n",
            "    self.send_eom(event.headers, write)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/h11/_writers.py\", line 96, in send_eom\n",
            "    raise LocalProtocolError(\"Too little data for declared Content-Length\")\n",
            "h11._util.LocalProtocolError: Too little data for declared Content-Length\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[1.84353757 1.49752593 1.14732158 0.9631691  1.06250882 1.45621419\n",
            " 0.43586376 1.2385515  0.96529663 0.97457856 0.43063128 1.00473404\n",
            " 0.90380496 0.93929005 0.89797676 0.96337062 0.42601952 1.092592\n",
            " 1.018121   0.62337577 0.43020326]\n",
            "INFO:     210.156.37.4:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.71613538 0.72379804 0.53988165 0.89436662 1.78000331 0.56009173\n",
            " 0.5838185  0.62110049 0.59766823 0.30198795 0.34405079 2.38570189\n",
            " 0.51206958 1.27987671 0.32060817 0.73808849 0.         0.40287337\n",
            " 1.35475826 0.73486519 0.        ]\n",
            "INFO:     2400:2413:5180:8500:9d65:e532:c765:19f2:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.53678364 1.04500699 0.84846926 1.09008288 1.18172848 1.01549697\n",
            " 1.11349988 0.64324737 0.74980772 0.84854829 0.66166866 0.86639017\n",
            " 0.87481177 1.12040401 0.46095031 0.66603762 0.53275341 0.55364895\n",
            " 1.15581501 0.74789608 0.48301297]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.44100689501895424\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1111 [0.32266666666666666, 0.3407473309608541] [0.7546666666666667, 0.9279359430604982]\n",
            "final text_encoder_type: bert-base-uncased\n",
            "0.44100689501895424\n",
            "0.44100689501895424\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n",
            "0.41207090998937823\n",
            "0.41207090998937823\n",
            "0.5120709099893782\n",
            "0.41207090998937823\n",
            "0.41207090998937823\n",
            "0.5120709099893782\n",
            "0.41207090998937823\n",
            "0.41207090998937823\n",
            "0.5120709099893782\n",
            "0.41207090998937823\n",
            "0.41207090998937823\n",
            "0.5120709099893782\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     2400:2413:5180:8500:9d65:e532:c765:19f2:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n",
            "[0.53678364 1.04500699 0.84846926 1.09008288 1.18172848 1.01549697\n",
            " 1.11349988 0.64324737 0.74980772 0.84854829 0.66166866 0.86639017\n",
            " 0.87481177 1.12040401 0.46095031 0.66603762 0.53275341 0.55364895\n",
            " 1.15581501 0.74789608 0.48301297]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.79002917 1.19908941 0.67732292 1.43827987 1.49497509 1.32207084\n",
            " 1.22033477 1.08607888 0.89780128 0.82949287 1.06508279 0.85599917\n",
            " 1.17399538 1.35575819 1.41372418 0.83992833 0.49687833 1.064466\n",
            " 1.38129711 0.64977098 0.48516467]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.18751040626053\n",
            "0.18751040626053\n",
            "0.28751040626053004\n",
            "0.28751040626053004\n",
            "0.28751040626053004\n",
            "0.18751040626053\n",
            "0.18751040626053\n",
            "0.28751040626053004\n",
            "0.28751040626053004\n",
            "0.28751040626053004\n",
            "0.18751040626053\n",
            "0.18751040626053\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.79002917 1.19908941 0.67732292 1.43827987 1.49497509 1.32207084\n",
            " 1.22033477 1.08607888 0.89780128 0.82949287 1.06508279 0.85599917\n",
            " 1.17399538 1.35575819 1.41372418 0.83992833 0.49687833 1.064466\n",
            " 1.38129711 0.64977098 0.48516467]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.84336662 0.68768167 0.74699771 0.71205175 0.86342311 0.32300416\n",
            " 0.30030546 0.         1.24165523 1.58249211 0.38840261 1.52775192\n",
            " 1.30365229 1.42605317 0.         1.341272   0.         0.32894593\n",
            " 0.45897505 0.69518232 0.82722855]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.305474751312502\n",
            "0.305474751312502\n",
            "0.405474751312502\n",
            "0.405474751312502\n",
            "0.405474751312502\n",
            "0.305474751312502\n",
            "0.305474751312502\n",
            "0.405474751312502\n",
            "0.405474751312502\n",
            "0.405474751312502\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.84336662 0.68768167 0.74699771 0.71205175 0.86342311 0.32300416\n",
            " 0.30030546 0.         1.24165523 1.58249211 0.38840261 1.52775192\n",
            " 1.30365229 1.42605317 0.         1.341272   0.         0.32894593\n",
            " 0.45897505 0.69518232 0.82722855]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.015054237181498953\n",
            "0.015054237181498953\n",
            "0.11505423718149899\n",
            "0.11505423718149899\n",
            "0.11505423718149899\n",
            "0.16505423718149898\n",
            "0.16505423718149898\n",
            "0.015054237181498953\n",
            "0.015054237181498953\n",
            "0.11505423718149899\n",
            "0.11505423718149899\n",
            "0.11505423718149899\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.46340606 0.         0.30977646 0.7619803  0.86062264 0.70171016\n",
            " 0.77749145 0.77534395 0.46860737 0.3883248  0.         0.65581459\n",
            " 0.36344737 0.42025805 0.37686253 0.61819059 0.6260497  0.71366155\n",
            " 0.47651148 0.357223   0.44957653]\n",
            "INFO:     240a:61:129c:a897:1974:7f3e:2bc5:859f:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.84336662 0.68768167 0.74699771 0.71205175 0.86342311 0.32300416\n",
            " 0.30030546 0.         1.24165523 1.58249211 0.38840261 1.52775192\n",
            " 1.30365229 1.42605317 0.         1.341272   0.         0.32894593\n",
            " 0.45897505 0.69518232 0.82722855]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.005\n",
            "0.005\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.1033464254621424\n",
            "0.1033464254621424\n",
            "0.1533464254621424\n",
            "0.20334642546214238\n",
            "0.1533464254621424\n",
            "0.05334642546214241\n",
            "0.005\n",
            "0.005\n",
            "0.05334642546214241\n",
            "lama\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "final text_encoder_type: bert-base-uncased\n",
            "[0.85772359 1.47918224 0.82358521 1.53750014 1.58167195 1.70028412\n",
            " 1.62408245 0.         0.71613586 1.56091702 1.33068657 1.35731196\n",
            " 1.58210337 0.93762034 0.         0.42224422 0.         0.95337117\n",
            " 1.5507741  0.         1.30809617]\n",
            "INFO:     240a:61:129c:a897:1974:7f3e:2bc5:859f:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2\n",
            "0.2\n",
            "0.30000000000000004\n",
            "0.30000000000000004\n",
            "0.30000000000000004\n",
            "0.2\n",
            "0.2\n",
            "0.30000000000000004\n",
            "0.30000000000000004\n",
            "0.30000000000000004\n",
            "0.2\n",
            "0.2\n",
            "0.30000000000000004\n",
            "0.30000000000000004\n",
            "0.30000000000000004\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     240a:61:129c:a897:1974:7f3e:2bc5:859f:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.85772359 1.47918224 0.82358521 1.53750014 1.58167195 1.70028412\n",
            " 1.62408245 0.         0.71613586 1.56091702 1.33068657 1.35731196\n",
            " 1.58210337 0.93762034 0.         0.42224422 0.         0.95337117\n",
            " 1.5507741  0.         1.30809617]\n",
            "INFO:     240a:61:129c:a897:1974:7f3e:2bc5:859f:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.005\n",
            "0.005\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.1033464254621424\n",
            "0.1033464254621424\n",
            "0.1533464254621424\n",
            "0.20334642546214238\n",
            "0.1533464254621424\n",
            "0.05334642546214241\n",
            "0.005\n",
            "0.005\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.1033464254621424\n",
            "0.1033464254621424\n",
            "0.1533464254621424\n",
            "0.20334642546214238\n",
            "0.1533464254621424\n",
            "0.05334642546214241\n",
            "0.005\n",
            "0.005\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.1033464254621424\n",
            "0.1033464254621424\n",
            "0.1533464254621424\n",
            "0.20334642546214238\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     240a:61:129c:a897:1974:7f3e:2bc5:859f:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.85772359 1.47918224 0.82358521 1.53750014 1.58167195 1.70028412\n",
            " 1.62408245 0.         0.71613586 1.56091702 1.33068657 1.35731196\n",
            " 1.58210337 0.93762034 0.         0.42224422 0.         0.95337117\n",
            " 1.5507741  0.         1.30809617]\n",
            "INFO:     240a:61:129c:a897:1974:7f3e:2bc5:859f:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.5066666666666667, 0.247] [0.8053333333333333, 0.877]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.005\n",
            "0.005\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.1033464254621424\n",
            "0.1033464254621424\n",
            "0.1533464254621424\n",
            "0.20334642546214238\n",
            "0.1533464254621424\n",
            "0.05334642546214241\n",
            "0.005\n",
            "0.005\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.1033464254621424\n",
            "0.1033464254621424\n",
            "0.1533464254621424\n",
            "0.20334642546214238\n",
            "0.1533464254621424\n",
            "0.05334642546214241\n",
            "0.005\n",
            "0.005\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.1033464254621424\n",
            "0.1033464254621424\n",
            "0.1533464254621424\n",
            "0.20334642546214238\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     240a:61:129c:a897:1974:7f3e:2bc5:859f:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.46471193 0.         0.         0.         0.         0.3591218\n",
            " 0.3026543  0.38327372 0.32227883 0.63220674 0.         0.74715245\n",
            " 0.31690317 0.44180173 0.         0.82041335 0.30882892 0.65413344\n",
            " 0.         0.31539389 0.4875432 ]\n",
            "INFO:     240a:61:129c:a897:1974:7f3e:2bc5:859f:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "{'text': 0.4353438846243219, 'Name tag': 0.4353438846243219, 'License plate': 0.5353438846243219, 'Mail': 0.5353438846243219, 'Documents': 0.5353438846243219, 'QR codes': 0.5853438846243219, 'barcodes': 0.5853438846243219, 'Map': 0.6353438846243219, 'Digital screens': 0.6853438846243218, 'information board': 0.6353438846243219, 'signboard': 0.5353438846243219, 'poster': 0.7853438846243219, 'sign': 0.5353438846243219, 'logo': 0.5353438846243219, 'card': 0.5853438846243219, 'window': 0.4853438846243219, 'mirror': 0.4853438846243219, 'Famous landmark': 0.7353438846243219, 'cardboard': 0.6853438846243218, 'manhole': 0.6853438846243218, 'utility pole': 0.7353438846243219}\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lama\n",
            "Inpainted image saved at /content/saved_images/output_simple_lama.jpg\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-simple-lama HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': 0.42628706341121664, 'Name tag': 0.42628706341121664, 'License plate': 0.5262870634112167, 'Mail': 0.5262870634112167, 'Documents': 0.5262870634112167, 'QR codes': 0.5762870634112166, 'barcodes': 0.5762870634112166, 'Map': 0.6262870634112166, 'Digital screens': 0.6762870634112166, 'information board': 0.6262870634112166, 'signboard': 0.5262870634112167, 'poster': 0.7762870634112167, 'sign': 0.5262870634112167, 'logo': 0.5262870634112167, 'card': 0.5762870634112166, 'window': 0.4762870634112166, 'mirror': 0.4762870634112166, 'Famous landmark': 0.7262870634112166, 'cardboard': 0.6762870634112166, 'manhole': 0.6762870634112166, 'utility pole': 0.7262870634112166}\n",
            "final text_encoder_type: bert-base-uncased\n",
            "lama\n",
            "Inpainted image saved at /content/saved_images/output_simple_lama.jpg\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-simple-lama HTTP/1.1\" 200 OK\n",
            "{'text': 0.41207090998937823, 'Name tag': 0.41207090998937823, 'License plate': 0.5120709099893782, 'Mail': 0.5120709099893782, 'Documents': 0.5120709099893782, 'QR codes': 0.5620709099893783, 'barcodes': 0.5620709099893783, 'Map': 0.6120709099893782, 'Digital screens': 0.6620709099893782, 'information board': 0.6120709099893782, 'signboard': 0.5120709099893782, 'poster': 0.7620709099893783, 'sign': 0.5120709099893782, 'logo': 0.5120709099893782, 'card': 0.5620709099893783, 'window': 0.4620709099893783, 'mirror': 0.4620709099893783, 'Famous landmark': 0.7120709099893783, 'cardboard': 0.6620709099893782, 'manhole': 0.6620709099893782, 'utility pole': 0.7120709099893783}\n",
            "final text_encoder_type: bert-base-uncased\n",
            "lama\n",
            "Inpainted image saved at /content/saved_images/output_simple_lama.jpg\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-simple-lama HTTP/1.1\" 200 OK\n",
            "{'text': 0.31552928931500246, 'Name tag': 0.31552928931500246, 'License plate': 0.41552928931500244, 'Mail': 0.41552928931500244, 'Documents': 0.41552928931500244, 'QR codes': 0.4655292893150025, 'barcodes': 0.4655292893150025, 'Map': 0.5155292893150025, 'Digital screens': 0.5655292893150025, 'information board': 0.5155292893150025, 'signboard': 0.41552928931500244, 'poster': 0.6655292893150024, 'sign': 0.41552928931500244, 'logo': 0.41552928931500244, 'card': 0.4655292893150025, 'window': 0.3655292893150025, 'mirror': 0.3655292893150025, 'Famous landmark': 0.6155292893150024, 'cardboard': 0.5655292893150025, 'manhole': 0.5655292893150025, 'utility pole': 0.6155292893150024}\n",
            "final text_encoder_type: bert-base-uncased\n",
            "lama\n",
            "Inpainted image saved at /content/saved_images/output_simple_lama.jpg\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-simple-lama HTTP/1.1\" 200 OK\n",
            "{'text': 0.08447071068499756, 'Name tag': 0.08447071068499756, 'License plate': 0.18447071068499757, 'Mail': 0.18447071068499757, 'Documents': 0.18447071068499757, 'QR codes': 0.23447071068499759, 'barcodes': 0.23447071068499759, 'Map': 0.2844707106849976, 'Digital screens': 0.33447071068499756, 'information board': 0.2844707106849976, 'signboard': 0.18447071068499757, 'poster': 0.4344707106849976, 'sign': 0.18447071068499757, 'logo': 0.18447071068499757, 'card': 0.23447071068499759, 'window': 0.1344707106849976, 'mirror': 0.1344707106849976, 'Famous landmark': 0.38447071068499755, 'cardboard': 0.33447071068499756, 'manhole': 0.33447071068499756, 'utility pole': 0.38447071068499755}\n",
            "final text_encoder_type: bert-base-uncased\n",
            "lama\n",
            "Inpainted image saved at /content/saved_images/output_simple_lama.jpg\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-simple-lama HTTP/1.1\" 200 OK\n",
            "{'text': 0.005, 'Name tag': 0.005, 'License plate': 0.0737129365887833, 'Mail': 0.0737129365887833, 'Documents': 0.0737129365887833, 'QR codes': 0.12371293658878335, 'barcodes': 0.12371293658878335, 'Map': 0.17371293658878334, 'Digital screens': 0.22371293658878333, 'information board': 0.17371293658878334, 'signboard': 0.0737129365887833, 'poster': 0.32371293658878336, 'sign': 0.0737129365887833, 'logo': 0.0737129365887833, 'card': 0.12371293658878335, 'window': 0.023712936588783373, 'mirror': 0.023712936588783373, 'Famous landmark': 0.2737129365887833, 'cardboard': 0.22371293658878333, 'manhole': 0.22371293658878333, 'utility pole': 0.2737129365887833}\n",
            "final text_encoder_type: bert-base-uncased\n",
            "lama\n",
            "Inpainted image saved at /content/saved_images/output_simple_lama.jpg\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-simple-lama HTTP/1.1\" 200 OK\n",
            "{'text': 0.005, 'Name tag': 0.005, 'License plate': 0.0737129365887833, 'Mail': 0.0737129365887833, 'Documents': 0.0737129365887833, 'QR codes': 0.12371293658878335, 'barcodes': 0.12371293658878335, 'Map': 0.17371293658878334, 'Digital screens': 0.22371293658878333, 'information board': 0.17371293658878334, 'signboard': 0.0737129365887833, 'poster': 0.32371293658878336, 'sign': 0.0737129365887833, 'logo': 0.0737129365887833, 'card': 0.12371293658878335, 'window': 0.023712936588783373, 'mirror': 0.023712936588783373, 'Famous landmark': 0.2737129365887833, 'cardboard': 0.22371293658878333, 'manhole': 0.22371293658878333, 'utility pole': 0.2737129365887833}\n",
            "final text_encoder_type: bert-base-uncased\n",
            "lama\n",
            "Inpainted image saved at /content/saved_images/output_simple_lama.jpg\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-simple-lama HTTP/1.1\" 200 OK\n",
            "{'text': 0.005, 'Name tag': 0.005, 'License plate': 0.05334642546214241, 'Mail': 0.05334642546214241, 'Documents': 0.05334642546214241, 'QR codes': 0.1033464254621424, 'barcodes': 0.1033464254621424, 'Map': 0.1533464254621424, 'Digital screens': 0.20334642546214238, 'information board': 0.1533464254621424, 'signboard': 0.05334642546214241, 'poster': 0.3033464254621424, 'sign': 0.05334642546214241, 'logo': 0.05334642546214241, 'card': 0.1033464254621424, 'window': 0.005, 'mirror': 0.005, 'Famous landmark': 0.25334642546214237, 'cardboard': 0.20334642546214238, 'manhole': 0.20334642546214238, 'utility pole': 0.25334642546214237}\n",
            "final text_encoder_type: bert-base-uncased\n",
            "lama\n",
            "Inpainted image saved at /content/saved_images/output_simple_lama.jpg\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-simple-lama HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 406, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 93, in __call__\n",
            "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 144, in simple_response\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 715, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 735, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 74, in app\n",
            "    await response(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 348, in __call__\n",
            "    await self._handle_simple(send, send_header_only)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 377, in _handle_simple\n",
            "    await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": more_body})\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 48, in sender\n",
            "    await send(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 48, in sender\n",
            "    await send(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 148, in send\n",
            "    await send(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 162, in _send\n",
            "    await send(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 510, in send\n",
            "    output = self.conn.send(event=h11.EndOfMessage())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/h11/_connection.py\", line 512, in send\n",
            "    data_list = self.send_with_data_passthrough(event)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/h11/_connection.py\", line 545, in send_with_data_passthrough\n",
            "    writer(event, data_list.append)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/h11/_writers.py\", line 67, in __call__\n",
            "    self.send_eom(event.headers, write)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/h11/_writers.py\", line 96, in send_eom\n",
            "    raise LocalProtocolError(\"Too little data for declared Content-Length\")\n",
            "h11._util.LocalProtocolError: Too little data for declared Content-Length\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': 0.005, 'Name tag': 0.005, 'License plate': 0.05334642546214241, 'Mail': 0.05334642546214241, 'Documents': 0.05334642546214241, 'QR codes': 0.1033464254621424, 'barcodes': 0.1033464254621424, 'Map': 0.1533464254621424, 'Digital screens': 0.20334642546214238, 'information board': 0.1533464254621424, 'signboard': 0.05334642546214241, 'poster': 0.3033464254621424, 'sign': 0.05334642546214241, 'logo': 0.05334642546214241, 'card': 0.1033464254621424, 'window': 0.005, 'mirror': 0.005, 'Famous landmark': 0.25334642546214237, 'cardboard': 0.20334642546214238, 'manhole': 0.20334642546214238, 'utility pole': 0.25334642546214237}\n",
            "final text_encoder_type: bert-base-uncased\n",
            "マスク画像のサイズを元画像に合わせてリサイズします: (800, 1200) -> (240, 360)\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-opencv HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n",
            "[0.71243465 0.55256826 0.33310312 0.72587699 0.77747142 0.93956077\n",
            " 0.70687354 0.41820621 0.50893885 0.5042724  0.         1.17945433\n",
            " 0.48941082 0.75126052 0.         0.50478637 0.36965156 0.82602257\n",
            " 0.77428126 0.39964125 0.        ]\n",
            "INFO:     210.137.33.142:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.009601461011058743\n",
            "0.009601461011058743\n",
            "0.10960146101105872\n",
            "0.10960146101105872\n",
            "0.10960146101105872\n",
            "0.15960146101105877\n",
            "0.15960146101105877\n",
            "0.009601461011058743\n",
            "0.009601461011058743\n",
            "0.10960146101105872\n",
            "0.10960146101105872\n",
            "0.10960146101105872\n",
            "0.15960146101105877\n",
            "lama\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     210.137.33.142:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n",
            "[0.73952568 0.59323007 0.63587087 0.66855258 0.71225369 0.5724197\n",
            " 0.51104808 0.42521417 1.16107321 0.74351287 0.31487912 1.08883846\n",
            " 0.54790449 0.67590582 0.42152843 1.21564293 0.         0.\n",
            " 0.93608528 0.         0.34632328]\n",
            "INFO:     210.156.37.4:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.18891811 1.52846313 0.30187491 0.91416228 0.99231732 0.79825604\n",
            " 0.76802522 0.77335209 0.78648674 0.6501056  0.         1.0422709\n",
            " 0.48064992 0.73167616 0.35055    0.96146154 0.         0.63492066\n",
            " 0.90221548 0.61871058 0.47965613]\n",
            "INFO:     210.156.37.4:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3660091925669623\n",
            "0.3660091925669623\n",
            "0.46600919256696227\n",
            "0.3660091925669623\n",
            "0.3660091925669623\n",
            "0.46600919256696227\n",
            "0.3660091925669623\n",
            "0.3660091925669623\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[1.18891811 1.52846313 0.30187491 0.91416228 0.99231732 0.79825604\n",
            " 0.76802522 0.77335209 0.78648674 0.6501056  0.         1.0422709\n",
            " 0.48064992 0.73167616 0.35055    0.96146154 0.         0.63492066\n",
            " 0.90221548 0.61871058 0.47965613]\n",
            "{'text': 0.2, 'Name tag': 0.2, 'License plate': 0.30000000000000004, 'Mail': 0.30000000000000004, 'Documents': 0.30000000000000004, 'QR codes': 0.35000000000000003, 'barcodes': 0.35000000000000003, 'Map': 0.4, 'Digital screens': 0.45, 'information board': 0.4, 'signboard': 0.30000000000000004, 'poster': 0.55, 'sign': 0.30000000000000004, 'logo': 0.30000000000000004, 'card': 0.35000000000000003, 'window': 0.25, 'mirror': 0.25, 'Famous landmark': 0.5, 'cardboard': 0.45, 'manhole': 0.45, 'utility pole': 0.5}\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "マスク画像のサイズを元画像に合わせてリサイズします: (800, 1200) -> (240, 360)\n",
            "INFO:     210.156.37.4:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-opencv HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n",
            "0.40012475544015746\n",
            "0.40012475544015746\n",
            "0.5001247554401574\n",
            "0.40012475544015746\n",
            "0.40012475544015746\n",
            "0.5001247554401574\n",
            "0.40012475544015746\n",
            "0.40012475544015746\n",
            "lama\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n",
            "[1.18891811 1.52846313 0.30187491 0.91416228 0.99231732 0.79825604\n",
            " 0.76802522 0.77335209 0.78648674 0.6501056  0.         1.0422709\n",
            " 0.48064992 0.73167616 0.35055    0.96146154 0.         0.63492066\n",
            " 0.90221548 0.61871058 0.47965613]\n",
            "INFO:     210.156.37.4:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.18891811 1.52846313 0.30187491 0.91416228 0.99231732 0.79825604\n",
            " 0.76802522 0.77335209 0.78648674 0.6501056  0.         1.0422709\n",
            " 0.48064992 0.73167616 0.35055    0.96146154 0.         0.63492066\n",
            " 0.90221548 0.61871058 0.47965613]\n",
            "INFO:     210.156.37.4:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.43221440536368194\n",
            "0.43221440536368194\n",
            "0.43221440536368194\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.71243465 0.55256826 0.33310312 0.72587699 0.77747142 0.93956077\n",
            " 0.70687354 0.41820621 0.50893885 0.5042724  0.         1.17945433\n",
            " 0.48941082 0.75126052 0.         0.50478637 0.36965156 0.82602257\n",
            " 0.77428126 0.39964125 0.        ]\n",
            "INFO:     210.137.33.142:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.44100689501895424\n",
            "0.44100689501895424\n",
            "lama\n",
            "INFO:     210.137.33.142:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 500 Internal Server Error\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 406, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 93, in __call__\n",
            "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 144, in simple_response\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 715, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 735, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 73, in app\n",
            "    response = await f(request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 301, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "  File \"<ipython-input-4-5bc1fe8770c8>\", line 765, in create_mask_sum\n",
            "    inpaint_image_with_mask1(input_path, mask_path, output_path, resize_factor=1)\n",
            "  File \"<ipython-input-4-5bc1fe8770c8>\", line 367, in inpaint_image_with_mask1\n",
            "    result = simple_lama(image, mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simple_lama_inpainting/models/model.py\", line 38, in __call__\n",
            "    inpainted = self.model(image, mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "RuntimeError: The following operation failed in the TorchScript interpreter.\n",
            "Traceback of TorchScript, serialized code (most recent call last):\n",
            "  File \"code/__torch__.py\", line 11, in forward\n",
            "    mask: Tensor) -> Tensor:\n",
            "    model = self.model\n",
            "    return (model).forward(mask, image, )\n",
            "            ~~~~~~~~~~~~~~ <--- HERE\n",
            "  File \"code/__torch__/saicinpainting/training/trainers/default.py\", line 13, in forward\n",
            "    masked_img = torch.mul(image, torch.rsub(mask, 1))\n",
            "    input = torch.cat([masked_img, mask], 1)\n",
            "    _0 = torch.mul(mask, (generator).forward(input, ))\n",
            "                          ~~~~~~~~~~~~~~~~~~ <--- HERE\n",
            "    _1 = torch.mul(torch.rsub(mask, 1), image)\n",
            "    return torch.add(_0, _1)\n",
            "  File \"code/__torch__/saicinpainting/training/modules/ffc.py\", line 10, in forward\n",
            "    input: Tensor) -> Tensor:\n",
            "    model = self.model\n",
            "    return (model).forward(input, )\n",
            "            ~~~~~~~~~~~~~~ <--- HERE\n",
            "class FFC_BN_ACT(Module):\n",
            "  __parameters__ = []\n",
            "  File \"code/__torch__/torch/nn/modules/container/___torch_mangle_818.py\", line 78, in forward\n",
            "    _1 = getattr(self, \"1\")\n",
            "    _0 = getattr(self, \"0\")\n",
            "    _29 = (_1).forward((_0).forward(input, ), )\n",
            "           ~~~~~~~~~~~ <--- HERE\n",
            "    _32 = (_4).forward((_3).forward((_2).forward(_29, ), ), )\n",
            "    _36, _37, = _32\n",
            "  File \"code/__torch__/saicinpainting/training/modules/ffc.py\", line 28, in forward\n",
            "    bn_l = self.bn_l\n",
            "    ffc = self.ffc\n",
            "    _0 = (bn_l).forward((ffc).forward(argument_1, ), )\n",
            "          ~~~~~~~~~~~~~ <--- HERE\n",
            "    _1 = (act_l).forward(_0, )\n",
            "    _2 = (bn_g).forward()\n",
            "  File \"code/__torch__/torch/nn/modules/batchnorm.py\", line 17, in forward\n",
            "    bias = self.bias\n",
            "    weight = self.weight\n",
            "    input = torch.batch_norm(argument_1, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)\n",
            "            ~~~~~~~~~~~~~~~~ <--- HERE\n",
            "    return input\n",
            "\n",
            "Traceback of TorchScript, original code (most recent call last):\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/functional.py(2450): batch_norm\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py(171): forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1190): _call_impl\n",
            "/home/enesmsahin/enes_workspace/lama/saicinpainting/training/modules/ffc.py(253): forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1190): _call_impl\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/container.py(204): forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1190): _call_impl\n",
            "/home/enesmsahin/enes_workspace/lama/saicinpainting/training/modules/ffc.py(367): forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1190): _call_impl\n",
            "/home/enesmsahin/enes_workspace/lama/saicinpainting/training/trainers/default.py(70): forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1190): _call_impl\n",
            "/home/enesmsahin/enes_workspace/lama/bin/to_jit.py(24): forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1190): _call_impl\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/jit/_trace.py(976): trace_module\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/jit/_trace.py(759): trace\n",
            "/home/enesmsahin/enes_workspace/lama/bin/to_jit.py(60): main\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/hydra/core/utils.py(160): run_job\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/hydra/_internal/hydra.py(97): run\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/hydra/_internal/utils.py(368): <lambda>\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/hydra/_internal/utils.py(211): run_and_report\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/hydra/_internal/utils.py(367): _run_hydra\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/hydra/main.py(49): decorated_main\n",
            "/home/enesmsahin/enes_workspace/lama/bin/to_jit.py(78): <module>\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 5.74 GiB. GPU 0 has a total capacity of 14.75 GiB of which 5.46 GiB is free. Process 134799 has 9.28 GiB memory in use. Of the allocated memory 6.39 GiB is allocated by PyTorch, and 2.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     210.156.37.4:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 422 Unprocessable Entity\n",
            "final text_encoder_type: bert-base-uncased\n",
            "[0.80692101 0.96020359 0.82344741 1.16844165 1.46694565 0.75214553\n",
            " 0.86753935 1.12434351 1.04674959 1.06647849 0.31298828 0.72859389\n",
            " 0.33475369 0.4403002  1.40777695 0.56514364 0.53143674 0.\n",
            " 1.40884638 0.45826778 0.3449299 ]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.44100689501895424\n",
            "0.44100689501895424\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.80692101 0.96020359 0.82344741 1.16844165 1.46694565 0.75214553\n",
            " 0.86753935 1.12434351 1.04674959 1.06647849 0.31298828 0.72859389\n",
            " 0.33475369 0.4403002  1.40777695 0.56514364 0.53143674 0.\n",
            " 1.40884638 0.45826778 0.3449299 ]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.53678364 1.04500699 0.84846926 1.09008288 1.18172848 1.01549697\n",
            " 1.11349988 0.64324737 0.74980772 0.84854829 0.66166866 0.86639017\n",
            " 0.87481177 1.12040401 0.46095031 0.66603762 0.53275341 0.55364895\n",
            " 1.15581501 0.74789608 0.48301297]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.44100689501895424\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.53678364 1.04500699 0.84846926 1.09008288 1.18172848 1.01549697\n",
            " 1.11349988 0.64324737 0.74980772 0.84854829 0.66166866 0.86639017\n",
            " 0.87481177 1.12040401 0.46095031 0.66603762 0.53275341 0.55364895\n",
            " 1.15581501 0.74789608 0.48301297]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.06111111111111111, 0.5108830845771144] [0.6277777777777778, 0.5631218905472637]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.23722125840582953\n",
            "0.23722125840582953\n",
            "0.3372212584058295\n",
            "0.3372212584058295\n",
            "0.3372212584058295\n",
            "0.23722125840582953\n",
            "0.23722125840582953\n",
            "0.3372212584058295\n",
            "0.3372212584058295\n",
            "0.3372212584058295\n",
            "0.23722125840582953\n",
            "0.23722125840582953\n",
            "lama\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "final text_encoder_type: bert-base-uncased\n",
            "[0.85772359 1.47918224 0.82358521 1.53750014 1.58167195 1.70028412\n",
            " 1.62408245 0.         0.71613586 1.56091702 1.33068657 1.35731196\n",
            " 1.58210337 0.93762034 0.         0.42224422 0.         0.95337117\n",
            " 1.5507741  0.         1.30809617]\n",
            "INFO:     49.105.84.110:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.22491699865623893\n",
            "0.22491699865623893\n",
            "0.3249169986562389\n",
            "0.3249169986562389\n",
            "0.3249169986562389\n",
            "0.22491699865623893\n",
            "0.22491699865623893\n",
            "0.3249169986562389\n",
            "0.3249169986562389\n",
            "0.3249169986562389\n",
            "0.22491699865623893\n",
            "0.22491699865623893\n",
            "0.3249169986562389\n",
            "0.3249169986562389\n",
            "0.3249169986562389\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n",
            "INFO:     49.105.84.110:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 406, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 93, in __call__\n",
            "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 144, in simple_response\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 715, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 735, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 74, in app\n",
            "    await response(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 348, in __call__\n",
            "    await self._handle_simple(send, send_header_only)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 377, in _handle_simple\n",
            "    await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": more_body})\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 48, in sender\n",
            "    await send(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 48, in sender\n",
            "    await send(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 148, in send\n",
            "    await send(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 162, in _send\n",
            "    await send(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 510, in send\n",
            "    output = self.conn.send(event=h11.EndOfMessage())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/h11/_connection.py\", line 512, in send\n",
            "    data_list = self.send_with_data_passthrough(event)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/h11/_connection.py\", line 545, in send_with_data_passthrough\n",
            "    writer(event, data_list.append)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/h11/_writers.py\", line 67, in __call__\n",
            "    self.send_eom(event.headers, write)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/h11/_writers.py\", line 96, in send_eom\n",
            "    raise LocalProtocolError(\"Too little data for declared Content-Length\")\n",
            "h11._util.LocalProtocolError: Too little data for declared Content-Length\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.53678364 1.04500699 0.84846926 1.09008288 1.18172848 1.01549697\n",
            " 1.11349988 0.64324737 0.74980772 0.84854829 0.66166866 0.86639017\n",
            " 0.87481177 1.12040401 0.46095031 0.66603762 0.53275341 0.55364895\n",
            " 1.15581501 0.74789608 0.48301297]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.53678364 1.04500699 0.84846926 1.09008288 1.18172848 1.01549697\n",
            " 1.11349988 0.64324737 0.74980772 0.84854829 0.66166866 0.86639017\n",
            " 0.87481177 1.12040401 0.46095031 0.66603762 0.53275341 0.55364895\n",
            " 1.15581501 0.74789608 0.48301297]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.06111111111111111, 0.5108830845771144] [0.6277777777777778, 0.5631218905472637]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.44100689501895424\n",
            "lama\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "final text_encoder_type: bert-base-uncased\n",
            "[0.53678364 1.04500699 0.84846926 1.09008288 1.18172848 1.01549697\n",
            " 1.11349988 0.64324737 0.74980772 0.84854829 0.66166866 0.86639017\n",
            " 0.87481177 1.12040401 0.46095031 0.66603762 0.53275341 0.55364895\n",
            " 1.15581501 0.74789608 0.48301297]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.32391039 0.31026199 0.         0.63615656 0.31262484 0.40544108\n",
            " 0.         0.         1.12569714 0.36287624 0.         0.50879067\n",
            " 0.3074559  0.47400829 0.         3.29279232 0.         0.54359806\n",
            " 0.         0.36103687 0.43896797]\n",
            "INFO:     130.34.47.9:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2\n",
            "0.2\n",
            "0.30000000000000004\n",
            "0.30000000000000004\n",
            "0.30000000000000004\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     130.34.47.9:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.32391039 0.31026199 0.         0.63615656 0.31262484 0.40544108\n",
            " 0.         0.         1.12569714 0.36287624 0.         0.50879067\n",
            " 0.3074559  0.47400829 0.         3.29279232 0.         0.54359806\n",
            " 0.         0.36103687 0.43896797]\n",
            "INFO:     130.34.47.9:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.027232632541767388\n",
            "0.027232632541767388\n",
            "0.12723263254176742\n",
            "0.12723263254176742\n",
            "0.12723263254176742\n",
            "0.1772326325417674\n",
            "lama\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     130.34.47.9:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n",
            "[0.32391039 0.31026199 0.         0.63615656 0.31262484 0.40544108\n",
            " 0.         0.         1.12569714 0.36287624 0.         0.50879067\n",
            " 0.3074559  0.47400829 0.         3.29279232 0.         0.54359806\n",
            " 0.         0.36103687 0.43896797]\n",
            "INFO:     130.34.47.9:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.46548387 0.40970361 0.65733701 0.82943499 0.50395989 0.69287884\n",
            " 0.57190627 0.8422302  0.43159682 0.58185136 0.32939675 0.67936051\n",
            " 0.36485174 0.40458882 0.46140811 0.33138856 0.         0.80542606\n",
            " 0.72290313 0.32938164 0.46440139]\n",
            "INFO:     130.34.47.9:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2\n",
            "0.2\n",
            "0.30000000000000004\n",
            "0.30000000000000004\n",
            "0.30000000000000004\n",
            "0.2\n",
            "0.2\n",
            "lama\n",
            "INFO:     130.34.47.9:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 500 Internal Server Error\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 406, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 93, in __call__\n",
            "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 144, in simple_response\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 715, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 735, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 73, in app\n",
            "    response = await f(request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 301, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "  File \"<ipython-input-4-5bc1fe8770c8>\", line 765, in create_mask_sum\n",
            "    inpaint_image_with_mask1(input_path, mask_path, output_path, resize_factor=1)\n",
            "  File \"<ipython-input-4-5bc1fe8770c8>\", line 367, in inpaint_image_with_mask1\n",
            "    result = simple_lama(image, mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simple_lama_inpainting/models/model.py\", line 38, in __call__\n",
            "    inpainted = self.model(image, mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "RuntimeError: The following operation failed in the TorchScript interpreter.\n",
            "Traceback of TorchScript, serialized code (most recent call last):\n",
            "  File \"code/__torch__.py\", line 11, in forward\n",
            "    mask: Tensor) -> Tensor:\n",
            "    model = self.model\n",
            "    return (model).forward(mask, image, )\n",
            "            ~~~~~~~~~~~~~~ <--- HERE\n",
            "  File \"code/__torch__/saicinpainting/training/trainers/default.py\", line 13, in forward\n",
            "    masked_img = torch.mul(image, torch.rsub(mask, 1))\n",
            "    input = torch.cat([masked_img, mask], 1)\n",
            "    _0 = torch.mul(mask, (generator).forward(input, ))\n",
            "                          ~~~~~~~~~~~~~~~~~~ <--- HERE\n",
            "    _1 = torch.mul(torch.rsub(mask, 1), image)\n",
            "    return torch.add(_0, _1)\n",
            "  File \"code/__torch__/saicinpainting/training/modules/ffc.py\", line 10, in forward\n",
            "    input: Tensor) -> Tensor:\n",
            "    model = self.model\n",
            "    return (model).forward(input, )\n",
            "            ~~~~~~~~~~~~~~ <--- HERE\n",
            "class FFC_BN_ACT(Module):\n",
            "  __parameters__ = []\n",
            "  File \"code/__torch__/torch/nn/modules/container/___torch_mangle_818.py\", line 78, in forward\n",
            "    _1 = getattr(self, \"1\")\n",
            "    _0 = getattr(self, \"0\")\n",
            "    _29 = (_1).forward((_0).forward(input, ), )\n",
            "           ~~~~~~~~~~~ <--- HERE\n",
            "    _32 = (_4).forward((_3).forward((_2).forward(_29, ), ), )\n",
            "    _36, _37, = _32\n",
            "  File \"code/__torch__/saicinpainting/training/modules/ffc.py\", line 28, in forward\n",
            "    bn_l = self.bn_l\n",
            "    ffc = self.ffc\n",
            "    _0 = (bn_l).forward((ffc).forward(argument_1, ), )\n",
            "          ~~~~~~~~~~~~~ <--- HERE\n",
            "    _1 = (act_l).forward(_0, )\n",
            "    _2 = (bn_g).forward()\n",
            "  File \"code/__torch__/torch/nn/modules/batchnorm.py\", line 17, in forward\n",
            "    bias = self.bias\n",
            "    weight = self.weight\n",
            "    input = torch.batch_norm(argument_1, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)\n",
            "            ~~~~~~~~~~~~~~~~ <--- HERE\n",
            "    return input\n",
            "\n",
            "Traceback of TorchScript, original code (most recent call last):\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/functional.py(2450): batch_norm\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py(171): forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1190): _call_impl\n",
            "/home/enesmsahin/enes_workspace/lama/saicinpainting/training/modules/ffc.py(253): forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1190): _call_impl\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/container.py(204): forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1190): _call_impl\n",
            "/home/enesmsahin/enes_workspace/lama/saicinpainting/training/modules/ffc.py(367): forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1190): _call_impl\n",
            "/home/enesmsahin/enes_workspace/lama/saicinpainting/training/trainers/default.py(70): forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1190): _call_impl\n",
            "/home/enesmsahin/enes_workspace/lama/bin/to_jit.py(24): forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1178): _slow_forward\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/nn/modules/module.py(1190): _call_impl\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/jit/_trace.py(976): trace_module\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/torch/jit/_trace.py(759): trace\n",
            "/home/enesmsahin/enes_workspace/lama/bin/to_jit.py(60): main\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/hydra/core/utils.py(160): run_job\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/hydra/_internal/hydra.py(97): run\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/hydra/_internal/utils.py(368): <lambda>\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/hydra/_internal/utils.py(211): run_and_report\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/hydra/_internal/utils.py(367): _run_hydra\n",
            "/home/enesmsahin/.conda/envs/sdiff/lib/python3.9/site-packages/hydra/main.py(49): decorated_main\n",
            "/home/enesmsahin/enes_workspace/lama/bin/to_jit.py(78): <module>\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 5.84 GiB. GPU 0 has a total capacity of 14.75 GiB of which 5.64 GiB is free. Process 134799 has 9.11 GiB memory in use. Of the allocated memory 6.49 GiB is allocated by PyTorch, and 2.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.69986105 1.37492537 0.33652511 0.69857037 0.788037   0.72620124\n",
            " 0.72974306 0.         0.         0.79685879 0.30367795 0.79624653\n",
            " 0.80067813 0.74419296 0.         1.07788825 0.         0.60030824\n",
            " 0.46488747 0.         0.45257375]\n",
            "INFO:     130.34.47.9:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.21248959373947002\n",
            "0.21248959373947002\n",
            "0.31248959373947005\n",
            "0.31248959373947005\n",
            "0.31248959373947005\n",
            "0.21248959373947002\n",
            "0.21248959373947002\n",
            "0.31248959373947005\n",
            "0.31248959373947005\n",
            "0.31248959373947005\n",
            "0.21248959373947002\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     130.34.47.9:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.80692101 0.96020359 0.82344741 1.16844165 1.46694565 0.75214553\n",
            " 0.86753935 1.12434351 1.04674959 1.06647849 0.31298828 0.72859389\n",
            " 0.33475369 0.4403002  1.40777695 0.56514364 0.53143674 0.\n",
            " 1.40884638 0.45826778 0.3449299 ]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.44100689501895424\n",
            "0.44100689501895424\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.80692101 0.96020359 0.82344741 1.16844165 1.46694565 0.75214553\n",
            " 0.86753935 1.12434351 1.04674959 1.06647849 0.31298828 0.72859389\n",
            " 0.33475369 0.4403002  1.40777695 0.56514364 0.53143674 0.\n",
            " 1.40884638 0.45826778 0.3449299 ]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.46860933 1.81304908 0.32862994 0.67180508 0.734644   1.68291414\n",
            " 1.32568717 0.         0.3742646  5.25717306 0.30435833 3.98876858\n",
            " 2.69388866 0.76835859 0.         0.70906955 0.         0.6248256\n",
            " 0.68341768 0.         0.30687618]\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.37741703 0.80045271 0.96102226 0.83356512 1.01056063 0.46626309\n",
            " 1.02906775 0.73326516 0.833722   0.77421856 0.33266595 0.75874865\n",
            " 0.47237569 0.66862261 0.68894696 0.80075622 0.4100647  0.71247864\n",
            " 0.80215394 0.39440894 0.54735261]\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.67067397 0.5490436  0.47895497 0.48633727 0.73646039 1.07415938\n",
            " 0.39095947 0.45455772 0.79522085 0.8281517  0.         0.62844676\n",
            " 0.58325702 0.49654633 0.3168785  0.52397174 0.         0.68136728\n",
            " 0.53204483 0.36594129 0.34016827]\n",
            "INFO:     130.34.47.9:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2\n",
            "0.2\n",
            "0.30000000000000004\n",
            "0.30000000000000004\n",
            "0.30000000000000004\n",
            "0.2\n",
            "0.2\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     130.34.47.9:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.67067397 0.5490436  0.47895497 0.48633727 0.73646039 1.07415938\n",
            " 0.39095947 0.45455772 0.79522085 0.8281517  0.         0.62844676\n",
            " 0.58325702 0.49654633 0.3168785  0.52397174 0.         0.68136728\n",
            " 0.53204483 0.36594129 0.34016827]\n",
            "INFO:     130.34.47.9:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.44100689501895424\n",
            "0.44100689501895424\n",
            "lama\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     130.34.47.9:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n",
            "[0.67067397 0.5490436  0.47895497 0.48633727 0.73646039 1.07415938\n",
            " 0.39095947 0.45455772 0.79522085 0.8281517  0.         0.62844676\n",
            " 0.58325702 0.49654633 0.3168785  0.52397174 0.         0.68136728\n",
            " 0.53204483 0.36594129 0.34016827]\n",
            "INFO:     130.34.47.9:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3903985389889413\n",
            "0.3903985389889413\n",
            "0.49039853898894126\n",
            "0.3903985389889413\n",
            "lama\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     130.34.47.9:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n",
            "[0.67067397 0.5490436  0.47895497 0.48633727 0.73646039 1.07415938\n",
            " 0.39095947 0.45455772 0.79522085 0.8281517  0.         0.62844676\n",
            " 0.58325702 0.49654633 0.3168785  0.52397174 0.         0.68136728\n",
            " 0.53204483 0.36594129 0.34016827]\n",
            "INFO:     130.34.47.9:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.31552928931500246\n",
            "0.31552928931500246\n",
            "0.41552928931500244\n",
            "0.41552928931500244\n",
            "0.41552928931500244\n",
            "0.31552928931500246\n",
            "0.31552928931500246\n",
            "lama\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     130.34.47.9:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "final text_encoder_type: bert-base-uncased\n",
            "[1.73475623 1.29585004 0.99521196 1.12151015 0.96355927 1.32483459\n",
            " 1.62044525 1.51785553 0.49839517 0.71932888 0.61805189 1.60241616\n",
            " 1.37505651 2.2095499  0.81029272 0.59916615 0.         0.94660461\n",
            " 0.73022395 1.05200744 0.80379707]\n",
            "INFO:     2400:2413:5180:8500:ac52:bfe5:c9e6:353a:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.68152308 0.59239388 0.75805593 0.98005033 0.76828533 0.56153923\n",
            " 0.43509662 0.         0.         0.         0.         0.\n",
            " 0.         0.36826935 0.         1.503039   0.         0.46212766\n",
            " 0.         0.         0.35793772]\n",
            "INFO:     240a:61:4080:84fe:9058:d77:8b42:ac6d:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.005\n",
            "0.005\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.1033464254621424\n",
            "0.1033464254621424\n",
            "0.1533464254621424\n",
            "0.20334642546214238\n",
            "0.1533464254621424\n",
            "0.05334642546214241\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     240a:61:4080:84fe:9058:d77:8b42:ac6d:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.77876854 0.         0.30496547 0.78215384 0.89132047 0.62793398\n",
            " 0.73934364 0.7932353  0.87010407 0.67820251 0.         0.80637556\n",
            " 0.54028261 0.48622176 0.32338029 0.73191071 0.33886409 0.47108495\n",
            " 0.84354645 0.34748825 0.32167697]\n",
            "INFO:     210.137.33.142:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.07693351463514558, 0.4696320923735063] [0.27706919984545764, 0.7181242516014918]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.21248959373947002\n",
            "0.21248959373947002\n",
            "0.31248959373947005\n",
            "0.31248959373947005\n",
            "0.31248959373947005\n",
            "0.21248959373947002\n",
            "0.21248959373947002\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     210.137.33.142:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.77876854 0.         0.30496547 0.78215384 0.89132047 0.62793398\n",
            " 0.73934364 0.7932353  0.87010407 0.67820251 0.         0.80637556\n",
            " 0.54028261 0.48622176 0.32338029 0.73191071 0.33886409 0.47108495\n",
            " 0.84354645 0.34748825 0.32167697]\n",
            "INFO:     210.137.33.142:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.07693351463514558, 0.4696320923735063] [0.27706919984545764, 0.7181242516014918]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.44100689501895424\n",
            "0.44100689501895424\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     210.137.33.142:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.77876854 0.         0.30496547 0.78215384 0.89132047 0.62793398\n",
            " 0.73934364 0.7932353  0.87010407 0.67820251 0.         0.80637556\n",
            " 0.54028261 0.48622176 0.32338029 0.73191071 0.33886409 0.47108495\n",
            " 0.84354645 0.34748825 0.32167697]\n",
            "INFO:     210.137.33.142:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.0762550885835852, 0.49617008996096107] [0.3035278158563125, 0.749487339659393]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.005\n",
            "0.005\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "0.1033464254621424\n",
            "0.1033464254621424\n",
            "0.1533464254621424\n",
            "0.20334642546214238\n",
            "0.1533464254621424\n",
            "0.05334642546214241\n",
            "0.005\n",
            "0.005\n",
            "0.05334642546214241\n",
            "0.05334642546214241\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     210.137.33.142:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "final text_encoder_type: bert-base-uncased\n",
            "[0.58552831 0.46228495 0.58182102 0.51741678 1.13575172 0.68388766\n",
            " 0.71755737 0.60434121 0.75826496 1.04264212 0.47483221 0.66592383\n",
            " 0.77476561 0.51710141 0.79674673 0.78932613 0.86282533 0.75460112\n",
            " 0.50507987 0.7004106  0.70862651]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.249343830056226\n",
            "0.249343830056226\n",
            "0.34934383005622605\n",
            "0.34934383005622605\n",
            "0.34934383005622605\n",
            "0.249343830056226\n",
            "0.249343830056226\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "[0.58552831 0.46228495 0.58182102 0.51741678 1.13575172 0.68388766\n",
            " 0.71755737 0.60434121 0.75826496 1.04264212 0.47483221 0.66592383\n",
            " 0.77476561 0.51710141 0.79674673 0.78932613 0.86282533 0.75460112\n",
            " 0.50507987 0.7004106  0.70862651]\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /classify-image/ HTTP/1.1\" 200 OK\n",
            "1111 [0.001, 0.001] [0.001, 0.001]\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.44100689501895424\n",
            "lama\n",
            "Inpainted image saved at /content/output_simple_lama.jpg\n",
            "INFO:     2400:2200:3b5:8a8d::446d:5d18:0 - \"POST /create-mask-and-inpaint-sum HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "dangerarray=[10,30,90,50,80,20,40,70,100,60]#ここに各クラスターの危険度を設定しておく\n",
        "#ここで認識する精度を上げたり下げたりできる\n",
        "\n",
        "thresholds = {\n",
        "    'text': 0.1,\n",
        "    'Name tag': 0.1,\n",
        "    'License plate': 0.3,\n",
        "    'Mail': 0.3,\n",
        "    'Documents': 0.3,\n",
        "    'QR codes': 0.4,\n",
        "    'barcodes': 0.4,\n",
        "    'Map': 0.5,\n",
        "    'Digital screens': 0.6,\n",
        "    'information board': 0.5,\n",
        "    'signboard': 0.3,\n",
        "    'poster': 0.8,\n",
        "    'sign': 0.3,\n",
        "    'logo': 0.3,\n",
        "    'card': 0.4,\n",
        "    'window': 0.2,\n",
        "    'mirror': 0.2,\n",
        "    'Famous landmark': 0.7,\n",
        "    'cardboard': 0.6,\n",
        "    'manhole': 0.6,\n",
        "    'utility pole': 0.7\n",
        "}\n",
        "\n",
        "# Define paths\n",
        "HOME = \"/content\"\n",
        "CONFIG_PATH = os.path.join(HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
        "WEIGHTS_NAME = \"groundingdino_swint_ogc.pth\"\n",
        "WEIGHTS_PATH = os.path.join(HOME, \"weights\", WEIGHTS_NAME)\n",
        "from PIL import Image\n",
        "\n",
        "def is_bright(pixel):\n",
        "    # ピクセルの輝度を計算して明るさを判定する\n",
        "    r, g, b = pixel\n",
        "    brightness = (0.299 * r + 0.587 * g + 0.114 * b)  # 輝度の計算\n",
        "    return brightness > 127  # 閾値を127に設定\n",
        "\n",
        "def analyze_mask_brightness(original_image_path, mask_image_path):\n",
        "    # 画像を開く\n",
        "    original_img = Image.open(original_image_path).convert('RGB')\n",
        "    mask_img = Image.open(mask_image_path).convert('L')  # グレースケールに変換\n",
        "\n",
        "    width, height = original_img.size\n",
        "\n",
        "    if mask_img.size != (width, height):\n",
        "        print(\"エラー: マスク画像と元画像のサイズが一致していません。\")\n",
        "        return\n",
        "\n",
        "    # 明るいピクセルと暗いピクセルのカウント\n",
        "    bright_count = 0\n",
        "    dark_count = 0\n",
        "\n",
        "    for y in range(height):\n",
        "        for x in range(width):\n",
        "            mask_value = mask_img.getpixel((x, y))\n",
        "            if mask_value > 127:  # マスクが白（対象領域）ならば\n",
        "                pixel = original_img.getpixel((x, y))\n",
        "                if is_bright(pixel):\n",
        "                    bright_count += 1\n",
        "                else:\n",
        "                    dark_count += 1\n",
        "\n",
        "    # 明るさの結果を判定\n",
        "    brightness_result = 1 if bright_count > dark_count else 2\n",
        "\n",
        "    return brightness_result\n",
        "\n",
        "def classify_mask_size(mask_image_path, small_threshold, medium_threshold, large_threshold):\n",
        "    # マスク画像を開く\n",
        "    mask_img = Image.open(mask_image_path).convert('L')  # グレースケールに変換\n",
        "\n",
        "    width, height = mask_img.size\n",
        "    total_pixels = width * height\n",
        "    white_pixel_count = 0\n",
        "\n",
        "    # マスク画像の白いピクセルをカウント\n",
        "    for y in range(height):\n",
        "        for x in range(width):\n",
        "            mask_value = mask_img.getpixel((x, y))\n",
        "            if mask_value > 127:  # 白いピクセルと判断\n",
        "                white_pixel_count += 1\n",
        "\n",
        "    # 白いピクセルの割合を計算\n",
        "    mask_area_ratio = (white_pixel_count / total_pixels) * 100\n",
        "\n",
        "    # マスクサイズを分類\n",
        "    if mask_area_ratio <= small_threshold:\n",
        "        size_category = 1  # すごく小さい\n",
        "    elif mask_area_ratio <= medium_threshold:\n",
        "        size_category = 2  # 小さい\n",
        "    elif mask_area_ratio <= large_threshold:\n",
        "        size_category = 3  # 大きい\n",
        "    else:\n",
        "        size_category = 4  # すごく大きい\n",
        "\n",
        "    return size_category\n",
        "\n",
        "def analyze_mask_combined(original_image_path, mask_image_path, small_threshold, medium_threshold, large_threshold):\n",
        "    # マスクの大きさを判定\n",
        "    size_category = classify_mask_size(mask_image_path, small_threshold, medium_threshold, large_threshold)\n",
        "\n",
        "    # マスク部分の明るさを判定\n",
        "    brightness_result = analyze_mask_brightness(original_image_path, mask_image_path)\n",
        "\n",
        "    # 結果を出力\n",
        "    size_text = {1: \"すごく小さい\", 2: \"小さい\", 3: \"大きい\", 4: \"すごく大きい\"}\n",
        "    print(f\"マスクの大きさ: {size_text[size_category]} ({size_category})\")\n",
        "    print(f\"マスクの明るさ: {brightness_result}\")\n",
        "    result={\n",
        "            'size':size_category,\n",
        "            'brightness':brightness_result\n",
        "            }\n",
        "    return result\n",
        "\n",
        "\n",
        "def quantize_model(model):\n",
        "    # モデルを動的量子化\n",
        "    model_quantized = torch.quantization.quantize_dynamic(\n",
        "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
        "    )\n",
        "    return model_quantized\n",
        "#この下で消去対象を決定\n",
        "def decide_to_object(risk_level):\n",
        "    tex = [\n",
        "        'text','Name tag', 'License plate', 'Mail', 'Documents', 'QR codes',\n",
        "        'barcodes', 'Map', 'Digital screens', 'information board',\n",
        "        'signboard', 'poster', 'sign', 'logo', 'card', 'window', 'mirror',\n",
        "        'Famous landmark', 'cardboard', 'manhole', 'utility pole'\n",
        "\n",
        "    ]\n",
        "    #この配列の要素の順番を変えると消える順番が変わる。\n",
        "    risk_level = int(risk_level / 20)*(len(tex)/10)#個数決定(1/2)\n",
        "    return tex[:int(risk_level)+1]\n",
        "\n",
        "def create_mask(image, x1, y1, x2, y2):\n",
        "    # Create a black image with the same size as the input image\n",
        "    mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
        "\n",
        "    # Draw a white rectangle on the mask where the object is located\n",
        "    cv2.rectangle(mask, (int(x1), int(y1)), (int(x2), int(y2)), 255, -1)\n",
        "\n",
        "    return mask\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#この下のコードは特定の領域をマスクしないタイプのコード\n",
        "def special_process_image(risk_level, image_path, point1, point2, thresholds=None):\n",
        "    if thresholds is None:\n",
        "        thresholds = {}\n",
        "\n",
        "    # Generate the threshold adjustments based on risk level\n",
        "    def logistic_decay(risk_level, k=0.1, r0=50):\n",
        "        return 1 / (1 + np.exp(-k * (risk_level - r0)))\n",
        "\n",
        "    threshold_results = {key: [] for key in thresholds.keys()}\n",
        "    decay_factor = logistic_decay(risk_level)\n",
        "\n",
        "    # Adjust thresholds for the given risk level\n",
        "    adjusted_thresholds = {key: max(value - decay_factor + 0.8, 0.01) / 2 for key, value in thresholds.items()}\n",
        "    detection_model = load_model(CONFIG_PATH, WEIGHTS_PATH)\n",
        "    IMAGE_PATH = image_path\n",
        "    LABELS = decide_to_object(risk_level)\n",
        "    image_source, image = load_image(IMAGE_PATH)\n",
        "\n",
        "    all_boxes = []\n",
        "    all_logits = []\n",
        "\n",
        "    for label in LABELS:\n",
        "        boxes, logits, _ = predict(\n",
        "            model=detection_model,\n",
        "            image=image,\n",
        "            caption=label,\n",
        "            box_threshold=0.3,\n",
        "            text_threshold=0.3\n",
        "        )\n",
        "        all_boxes.extend(boxes)\n",
        "        all_logits.extend(logits)\n",
        "\n",
        "    # Convert PyTorch tensor to NumPy array and ensure it's uint8\n",
        "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "    image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)  # Ensure values are in [0, 255]\n",
        "\n",
        "    # Create a copy of the image for debugging (to draw the rectangle on)\n",
        "    debug_image = image_np.copy()\n",
        "\n",
        "    # Initialize mask as black\n",
        "    mask = np.zeros((image_np.shape[0], image_np.shape[1]), dtype=np.uint8)\n",
        "\n",
        "    for i, box in enumerate(all_boxes):\n",
        "        x, y, w, h = box\n",
        "        confidence = all_logits[i]  # Get the confidence score\n",
        "        object_type = LABELS[i % len(LABELS)]  # Determine the object type\n",
        "\n",
        "        # Use the threshold specific to the object type, default to 0.5 if not provided\n",
        "        threshold = adjusted_thresholds.get(object_type, 0.5)\n",
        "        print(threshold)\n",
        "        if confidence >= threshold:\n",
        "            x1 = int((x - w / 2) * image_np.shape[1])\n",
        "            y1 = int((y - h / 2) * image_np.shape[0])\n",
        "            x2 = int((x + w / 2) * image_np.shape[1])\n",
        "            y2 = int((y + h / 2) * image_np.shape[0])\n",
        "\n",
        "            # Ensure coordinates are within the image dimensions\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(image_np.shape[1], x2), min(image_np.shape[0], y2)\n",
        "\n",
        "            mask = create_mask(image_np, x1, y1, x2, y2) | mask\n",
        "\n",
        "    # Convert the two points from relative (0-1) values to absolute pixel coordinates\n",
        "    p1_x, p1_y = int(point1[0] * image_np.shape[1]), int(point1[1] * image_np.shape[0])\n",
        "    p2_x, p2_y = int(point2[0] * image_np.shape[1]), int(point2[1] * image_np.shape[0])\n",
        "\n",
        "    # Ensure coordinates are within the image dimensions\n",
        "    x_min = max(0, min(p1_x, p2_x))\n",
        "    x_max = min(image_np.shape[1], max(p1_x, p2_x))\n",
        "    y_min = max(0, min(p1_y, p2_y))\n",
        "    y_max = min(image_np.shape[0], max(p1_y, p2_y))\n",
        "\n",
        "    # Set the area defined by the rectangle to 0 (black)\n",
        "    mask[y_min:y_max, x_min:x_max] = 0\n",
        "\n",
        "    # Draw a white rectangle for debugging on the original image\n",
        "    cv2.rectangle(debug_image, (x_min, y_min), (x_max, y_max), (255, 255, 255), 2)  # 2 is the line thickness\n",
        "\n",
        "    # Save the debug image with the white rectangle\n",
        "    debug_image_pil = Image.fromarray(debug_image)\n",
        "    debug_image_pil.save(\"/content/debug_image_with_rectangle.jpg\")\n",
        "\n",
        "    # Save the mask as an image\n",
        "    final_image_pil = Image.fromarray(mask)\n",
        "    final_image_pil.save(\"/content/final_mask.jpg\")\n",
        "\n",
        "    return \"/content/final_mask.jpg\"\n",
        "\n",
        "#YOLOの本番バージョンコード、マスク画像作成。速度がめっちゃ早い\n",
        "def special_process_image_yolo(risk_level, image_path, point1, point2, thresholds=None):\n",
        "    if thresholds is None:\n",
        "        thresholds = {\n",
        "            'text': 0.01,\n",
        "            'Name tag': 0.1,\n",
        "            'License plate': 0.3,\n",
        "            'Mail': 0.3,\n",
        "            'Documents': 0.3,\n",
        "            'QR codes': 0.4,\n",
        "            'barcodes': 0.4,\n",
        "            'Map': 0.5,\n",
        "            'Digital screens': 0.6,\n",
        "            'information board': 0.5,\n",
        "            'signboard': 0.3,\n",
        "            'poster': 0.8,\n",
        "            'sign': 0.3,\n",
        "            'logo': 0.3,\n",
        "            'card': 0.4,\n",
        "            'window': 0.2,\n",
        "            'mirror': 0.2,\n",
        "            'Famous landmark': 0.7,\n",
        "            'cardboard': 0.6,\n",
        "            'manhole': 0.6,\n",
        "            'utility pole': 0.7\n",
        "        }\n",
        "\n",
        "    # タイムスタンプを作成\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "    # Generate the threshold adjustments based on risk level\n",
        "    def logistic_decay(risk_level, k=0.1, r0=50):\n",
        "        return 1 / (1 + np.exp(-k * (risk_level - r0)))\n",
        "\n",
        "    threshold_results = {key: [] for key in thresholds.keys()}\n",
        "    decay_factor = logistic_decay(risk_level)\n",
        "\n",
        "    # Adjust thresholds for the given risk level\n",
        "    adjusted_thresholds = {key: max(value - decay_factor + 0.8, 0.01) / 2 for key, value in thresholds.items()}\n",
        "\n",
        "    # YOLOv8モデルを読み込む\n",
        "    model = YOLO('/content/drive/MyDrive/学習データ/yolov8n.pt')  # モデルのパスを適宜変更してください\n",
        "\n",
        "    # 画像を読み込む\n",
        "    image = cv2.imread(image_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # 推論を実行する\n",
        "    results = model(image_rgb)\n",
        "\n",
        "    # 検出結果からボックスと信頼スコアを取得\n",
        "    detections = results[0].boxes  # ボックス情報を含む結果のリスト\n",
        "\n",
        "    # Convert image to NumPy array and ensure it's uint8\n",
        "    image_np = np.array(image_rgb, dtype=np.uint8)\n",
        "\n",
        "    # Create a copy of the image for debugging (to draw the rectangle on)\n",
        "    debug_image = image_np.copy()\n",
        "\n",
        "    # Initialize mask as black\n",
        "    mask = np.zeros((image_np.shape[0], image_np.shape[1]), dtype=np.uint8)\n",
        "\n",
        "    for box in detections:\n",
        "        x, y, w, h = box.xywh[0]  # ボックスの中心座標と幅・高さ\n",
        "        confidence = box.conf[0]  # ボックスの信頼度\n",
        "        class_id = box.cls[0]  # オブジェクトのクラスID\n",
        "        object_type = model.names[int(class_id)]  # オブジェクトのタイプを取得\n",
        "\n",
        "        # クラス名に基づいたしきい値を使用する\n",
        "        threshold = adjusted_thresholds.get(object_type, 0.5)  # デフォルト値は0.5\n",
        "\n",
        "        if confidence >= threshold:\n",
        "            x1 = int((x - w / 2) * image_np.shape[1])\n",
        "            y1 = int((y - h / 2) * image_np.shape[0])\n",
        "            x2 = int((x + w / 2) * image_np.shape[1])\n",
        "            y2 = int((y + h / 2) * image_np.shape[0])\n",
        "\n",
        "            # Ensure coordinates are within the image dimensions\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(image_np.shape[1], x2), min(image_np.shape[0], y2)\n",
        "\n",
        "            mask = create_mask(image_np, x1, y1, x2, y2) | mask\n",
        "\n",
        "    # Convert the two points from relative (0-1) values to absolute pixel coordinates\n",
        "    p1_x, p1_y = int(point1[0] * image_np.shape[1]), int(point1[1] * image_np.shape[0])\n",
        "    p2_x, p2_y = int(point2[0] * image_np.shape[1]), int(point2[1] * image_np.shape[0])\n",
        "\n",
        "    # Ensure coordinates are within the image dimensions\n",
        "    x_min = max(0, min(p1_x, p2_x))\n",
        "    x_max = min(image_np.shape[1], max(p1_x, p2_x))\n",
        "    y_min = max(0, min(p1_y, p2_y))\n",
        "    y_max = min(image_np.shape[0], max(p1_y, p2_y))\n",
        "\n",
        "    # Set the area defined by the rectangle to 0 (black)\n",
        "    mask[y_min:y_max, x_min:x_max] = 0\n",
        "\n",
        "    # Draw a white rectangle for debugging on the original image\n",
        "    cv2.rectangle(debug_image, (x_min, y_min), (x_max, y_max), (255, 255, 255), 2)  # 2 is the line thickness\n",
        "\n",
        "    # Save the debug image with the white rectangle, including the timestamp in the filename\n",
        "    debug_image_pil = Image.fromarray(debug_image)\n",
        "    debug_image_pil.save(f\"/content/debug_image_with_rectangle_{timestamp}.jpg\")\n",
        "\n",
        "    # Save the mask as an image, including the timestamp in the filename\n",
        "    final_image_pil = Image.fromarray(mask)\n",
        "    final_image_pil.save(f\"/content/final_mask_{timestamp}.jpg\")\n",
        "\n",
        "    return f\"/content/final_mask_{timestamp}.jpg\"\n",
        "\n",
        "\n",
        "#この下のコードは普通タイプのコード\n",
        "\n",
        "def process_image(risk_level, image_path, thresholds=None):\n",
        "    if thresholds is None:\n",
        "        thresholds = {}\n",
        "\n",
        "    # Generate the threshold adjustments based on risk level\n",
        "    def logistic_decay(risk_level, k=0.1, r0=50):\n",
        "        return 1 / (1 + np.exp(-k * (risk_level - r0)))\n",
        "\n",
        "    threshold_results = {key: [] for key in thresholds.keys()}\n",
        "    decay_factor = logistic_decay(risk_level)\n",
        "\n",
        "    # Adjust thresholds for the given risk level\n",
        "    adjusted_thresholds = {key: max(value - decay_factor + 0.8, 0.01) / 2 for key, value in thresholds.items()}\n",
        "    print(adjusted_thresholds)\n",
        "    detection_model = load_model(CONFIG_PATH, WEIGHTS_PATH)\n",
        "    risk_level = int(risk_level)\n",
        "    IMAGE_PATH = image_path\n",
        "    LABELS = decide_to_object(risk_level)\n",
        "    image_source, image = load_image(IMAGE_PATH)\n",
        "\n",
        "    all_boxes = []\n",
        "    all_logits = []\n",
        "\n",
        "    for label in LABELS:\n",
        "        boxes, logits, _ = predict(\n",
        "            model=detection_model,\n",
        "            image=image,\n",
        "            caption=label,\n",
        "            box_threshold=0.3,\n",
        "            text_threshold=0.3\n",
        "        )\n",
        "        all_boxes.extend(boxes)\n",
        "        all_logits.extend(logits)\n",
        "\n",
        "    # Convert PyTorch tensor to NumPy array and ensure it's uint8\n",
        "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "    image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)  # Ensure values are in [0, 255]\n",
        "\n",
        "    # Initialize mask as black\n",
        "    mask = np.zeros((image_np.shape[0], image_np.shape[1]), dtype=np.uint8)\n",
        "\n",
        "    for i, box in enumerate(all_boxes):\n",
        "        x, y, w, h = box\n",
        "        confidence = all_logits[i]  # Get the confidence score\n",
        "        object_type = LABELS[i % len(LABELS)]  # Determine the object type\n",
        "\n",
        "        # Use the threshold specific to the object type, default to the adjusted threshold if not provided\n",
        "        threshold = adjusted_thresholds.get(object_type, 0.5)\n",
        "\n",
        "        if confidence >= threshold:\n",
        "            # Adjust the box coordinates, convert to integers\n",
        "            x1 = int((x - w / 2) * image_np.shape[1])\n",
        "            y1 = int((y - h / 2) * image_np.shape[0])\n",
        "            x2 = int((x + w / 2) * image_np.shape[1])\n",
        "            y2 = int((y + h / 2) * image_np.shape[0])\n",
        "\n",
        "            # Ensure coordinates are within the image dimensions\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(image_np.shape[1], x2), min(image_np.shape[0], y2)\n",
        "\n",
        "            mask = create_mask(image_np, x1, y1, x2, y2) | mask\n",
        "\n",
        "    # Save the mask as an image\n",
        "    final_image_pil = Image.fromarray(mask)\n",
        "    final_image_pil.save(\"/content/final_mask.jpg\")\n",
        "\n",
        "    return \"/content/final_mask.jpg\"\n",
        "\n",
        "\n",
        "#この下は、openCV\n",
        "def inpaint_image_with_mask(image_path, mask_path, output_path, inpaint_radius=5, inpaint_method=cv2.INPAINT_TELEA):\n",
        "    \"\"\"\n",
        "    マスク画像を使用して元画像のインペイントを行う関数。\n",
        "\n",
        "    Parameters:\n",
        "    - image_path: 元画像のパス\n",
        "    - mask_path: マスク画像のパス（修復したい領域が白、その他が黒）\n",
        "    - output_path: インペイント結果の出力パス\n",
        "    - inpaint_radius: インペイントの半径（デフォルトは5）\n",
        "    - inpaint_method: インペイントのアルゴリズム（デフォルトはcv2.INPAINT_TELEA）\n",
        "\n",
        "    Returns:\n",
        "    - inpainted_image: インペイントされた画像\n",
        "    \"\"\"\n",
        "    # 画像とマスクを読み込み\n",
        "    image = cv2.imread(image_path)\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # マスクはグレースケールで読み込み\n",
        "\n",
        "    # マスク画像が正常に読み込めたかチェック\n",
        "    if image is None:\n",
        "        raise ValueError(f\"元画像が見つかりません: {image_path}\")\n",
        "    if mask is None:\n",
        "        raise ValueError(f\"マスク画像が見つかりません: {mask_path}\")\n",
        "\n",
        "    # マスク画像が元画像と同じサイズでない場合、リサイズ\n",
        "    if image.shape[:2] != mask.shape[:2]:\n",
        "        print(f\"マスク画像のサイズを元画像に合わせてリサイズします: {mask.shape} -> {image.shape[:2]}\")\n",
        "        mask = cv2.resize(mask, (image.shape[1], image.shape[0]))\n",
        "\n",
        "    # インペイント処理\n",
        "    inpainted_image = cv2.inpaint(image, mask, inpaint_radius, inpaint_method)\n",
        "\n",
        "    # インペイント結果を保存\n",
        "    cv2.imwrite(output_path, inpainted_image)\n",
        "\n",
        "    return inpainted_image\n",
        "\n",
        "#この下はlama-simpleのinpaint\n",
        "def inpaint_image_with_mask1(img_path, mask_path, output_path, resize_factor=0.5):\n",
        "    print('lama')\n",
        "    # 画像とマスクを読み込み\n",
        "    image = Image.open(img_path)\n",
        "    mask = Image.open(mask_path).convert('L')  # マスクをグレースケールに変換\n",
        "\n",
        "    # 画像とマスクのサイズを合わせる\n",
        "    mask = mask.resize(image.size, Image.NEAREST)\n",
        "   #マスクのエッジをぼかす (Gaussian Blur)\n",
        "    blurred_mask = mask.filter(ImageFilter.GaussianBlur(radius=3))  #半径3ピクセルでぼかし\n",
        "    # SimpleLama インスタンスを作成\n",
        "    simple_lama = SimpleLama()\n",
        "\n",
        "    # インペイントの実行\n",
        "    result = simple_lama(image, mask)\n",
        "\n",
        "    # 出力画像をリサイズ\n",
        "    new_size = (int(result.width * resize_factor), int(result.height * resize_factor))\n",
        "    result = result.resize(new_size, Image.ANTIALIAS)\n",
        "\n",
        "    # 結果を保存\n",
        "    result.save(output_path)\n",
        "    print(f\"Inpainted image saved at {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "\n",
        "\n",
        "#この下はStable Diffusion\n",
        "def load_inpainting_pipeline():\n",
        "    pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-2-inpainting\", torch_dtype=torch.float16\n",
        "    )\n",
        "    pipe = pipe.to(\"cuda\")\n",
        "    return pipe\n",
        "\n",
        "def inpaint_images(image_paths, mask_paths, output_dir=\"output_images\"):\n",
        "    #パイプラインの読み込み\n",
        "    pipe = load_inpainting_pipeline()\n",
        "\n",
        "    #出力ディレクトリの作成\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    #並列処理でインペインティングを実行\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for i, (image_path, mask_path) in enumerate(zip(image_paths, mask_paths)):\n",
        "            #画像とマスクを読み込み\n",
        "            image = cv2.imread(image_path)\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            #画像とマスクの読み込みチェック\n",
        "            if image is None:\n",
        "                raise ValueError(f\"{image_path} が正しく読み込まれていません。ファイルパスを確認してください。\")\n",
        "            if mask is None:\n",
        "                raise ValueError(f\"{mask_path} が正しく読み込まれていません。ファイルパスを確認してください。\")\n",
        "\n",
        "            #マスクと画像のサイズ確認\n",
        "            if mask.shape[:2] != image.shape[:2]:\n",
        "                raise ValueError(f\"マスク {mask_path} のサイズが画像 {image_path} のサイズと一致しません。\")\n",
        "\n",
        "            #画像とマスクをPIL形式に変換\n",
        "            image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "            mask_pil = Image.fromarray(mask)\n",
        "\n",
        "            #インペインティング処理の実行を非同期で開始\n",
        "            futures.append(executor.submit(\n",
        "                lambda img, m, idx: pipe(\n",
        "                    prompt=\"Fill the masked area with contextually appropriate background or scenery that matches the surrounding area.\",\n",
        "                    negative_prompt=\"Exclude people, human figures, arms, and any human body parts from the generated content.\",\n",
        "                    image=img,\n",
        "                    mask_image=m,\n",
        "                    num_inference_steps=100,  #ステップ数を増加\n",
        "                    guidance_scale=7.5  #高いガイダンススケールを設定\n",
        "                ).images[0], image_pil, mask_pil, i\n",
        "            ))\n",
        "\n",
        "        #結果を取得して保存\n",
        "        for i, future in enumerate(futures):\n",
        "            result = future.result()\n",
        "\n",
        "            #結果のリサイズと保存\n",
        "            result_image = result.resize((image.shape[1], image.shape[0]), Image.LANCZOS)\n",
        "            output_image_path = os.path.join(output_dir, f\"output_{i+1}.png\")\n",
        "            result_image.save(output_image_path)\n",
        "\n",
        "            #処理が完了したことを通知\n",
        "            print(f\"{image_paths[i]} と {mask_paths[i]} のペアの処理が完了しました。結果は {output_image_path} に保存されました。\")\n",
        "#下記はDeepFillv2のコード\n",
        "\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "# CORSミドルウェアの追加\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # ここを適切なオリジンに設定することもできます\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# 保存先のディレクトリを指定\n",
        "SAVE_DIR = Path(\"/content/saved_images\")\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_image(file, filename):\n",
        "    \"\"\"画像ファイルを指定ディレクトリに保存\"\"\"\n",
        "    filepath = SAVE_DIR / filename\n",
        "    with open(filepath, \"wb\") as buffer:\n",
        "        shutil.copyfileobj(file, buffer)\n",
        "    return filepath\n",
        "\n",
        "@app.post(\"/create-mask-and-inpaint-opencv\")\n",
        "async def create_mask_and_inpaint_opencv(image: UploadFile = File(...), risk_level: int = Form(...)):\n",
        "    input_path = save_image(image.file, \"input.jpg\")\n",
        "    mask_path = process_image(risk_level, input_path, thresholds=thresholds)\n",
        "\n",
        "    output_path = SAVE_DIR / \"output_opencv.jpg\"\n",
        "    # OpenCVでインペイント\n",
        "    inpaint_image_with_mask(input_path, mask_path, output_path)\n",
        "\n",
        "    return FileResponse(output_path)\n",
        "\n",
        "@app.post(\"/create-mask-and-inpaint-simple-lama\")\n",
        "async def create_mask_and_inpaint_simple_lama(image: UploadFile = File(...), risk_level: int = Form(...)):\n",
        "    input_path = save_image(image.file, \"input.jpg\")\n",
        "    mask_path =  process_image(risk_level, input_path, thresholds=thresholds)# マスク画像を作成\n",
        "    output_path = SAVE_DIR / \"output_simple_lama.jpg\"\n",
        "    # SimpleLamaでインペイント\n",
        "    inpaint_image_with_mask1(input_path, mask_path, output_path, resize_factor=1)\n",
        "\n",
        "    return FileResponse(output_path)\n",
        "\n",
        "\n",
        "#下のendpointは特定領域をマスクしないタイプのもの\n",
        "\n",
        "\n",
        "#下記はDeepFillv2\n",
        "\n",
        "\n",
        "\n",
        "# マスクを画像サイズにリサイズする関数\n",
        "def resize_mask_to_image(mask_image_path, target_image_path):\n",
        "    \"\"\"\n",
        "    マスク画像のサイズを元画像のサイズに合わせる\n",
        "    \"\"\"\n",
        "    # 元画像とマスクを読み込む\n",
        "    target_image = Image.open(target_image_path)\n",
        "    mask_image = Image.open(mask_image_path)\n",
        "\n",
        "    # マスクを元画像のサイズにリサイズ\n",
        "    resized_mask = mask_image.resize(target_image.size, Image.NEAREST)\n",
        "\n",
        "    return resized_mask\n",
        "\n",
        "\n",
        "class StopExecution(Exception):\n",
        "    def _render_traceback_(self):\n",
        "        pass\n",
        "\n",
        "def copy_image_to_folder(image_path, save_as_input):\n",
        "    \"\"\"\n",
        "    Copies the given image to the appropriate folder based on the save_as_input flag.\n",
        "\n",
        "    Parameters:\n",
        "    image_path (str): The path of the image to be copied.\n",
        "    save_as_input (int): 0 to save as input image, 1 to save as mask image.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"Error: The file {image_path} does not exist!\")\n",
        "        raise StopExecution\n",
        "\n",
        "    # Set the destination folder and file name based on the save_as_input flag\n",
        "    if save_as_input == 0:\n",
        "        destination_path = \"/content/deepfillv2_colab/input/input_img.png\"\n",
        "        print(f\"Saving image as input: {destination_path}\")\n",
        "    elif save_as_input == 1:\n",
        "        destination_path = \"/content/deepfillv2_colab/input/mask.png\"\n",
        "        print(f\"Saving image as mask: {destination_path}\")\n",
        "    else:\n",
        "        print(\"Error: Invalid save_as_input value. It must be 0 (input) or 1 (mask).\")\n",
        "        raise StopExecution\n",
        "\n",
        "    # Copy the image to the destination\n",
        "    shutil.copy(image_path, destination_path)\n",
        "    print(f\"Image {image_path} copied to {destination_path} successfully!\")\n",
        "\n",
        "\n",
        "def run_python_script(script_path):\n",
        "    try:\n",
        "        # Pythonスクリプトを新しいプロセスとして実行\n",
        "        result = subprocess.run(['python', script_path], check=True, text=True, capture_output=True)\n",
        "        # 実行が成功した場合、標準出力を表示\n",
        "        print(\"Script output:\", result.stdout)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error occurred while running script: {e}\")\n",
        "        print(\"Script stderr:\", e.stderr)\n",
        "\n",
        "\n",
        "\n",
        "@app.post(\"/create-mask-and-inpaint-deepfillv2\")\n",
        "# エンドポイントの実装\n",
        "async def deepfillv2_inpaint_endpoint(\n",
        "    image: UploadFile = File(...),\n",
        "    risk_level: int = Form(...)\n",
        "):\n",
        "    input_path = save_image(image.file, \"input.jpg\")\n",
        "    save_as_input=0\n",
        "    copy_image_to_folder(input_path, save_as_input)\n",
        "    input_path=process_image(risk_level,input_path, thresholds=thresholds)\n",
        "    save_as_input=1#option\n",
        "    copy_image_to_folder(input_path, save_as_input)\n",
        "    os.chdir(f\"{HOME}/deepfillv2_colab\")#一度ディレクトリを変更\n",
        "    run_python_script('/content/deepfillv2_colab/inpaint.py')#プロセス用のコードを呼び出す\n",
        "    #!python inpaint.py\n",
        "    os.chdir(f\"{HOME}/GroundingDINO\") #元に戻す\n",
        "    output_image_path='/content/deepfillv2_colab/output/inpainted_img.png'\n",
        "    return FileResponse(output_image_path)\n",
        "\n",
        "\n",
        "@app.post(\"/create-mask-and-inpaint-stable-diffusion\")\n",
        "async def create_mask_and_inpaint_stable_diffusion(image: UploadFile = File(...), risk_level: int = Form(...)):\n",
        "    input_path = save_image(image.file, \"input.jpg\")\n",
        "    mask_path = process_image(risk_level, input_path, thresholds=thresholds) # マスク画像を作成\n",
        "\n",
        "    output_path = SAVE_DIR / \"output_stable_diffusion.png\"\n",
        "\n",
        "    # 保存された画像の有効性を確認\n",
        "    validate_image(input_path)\n",
        "\n",
        "\n",
        "    # マスクと元画像のサイズが一致しない場合はリサイズ\n",
        "    resize_mask_if_needed(mask_path, input_path)\n",
        "\n",
        "    # Stable Diffusionでインペイント\n",
        "    inpaint_images([input_path], [mask_path], output_dir=SAVE_DIR)\n",
        "\n",
        "    return FileResponse(SAVE_DIR / \"output_1.png\")\n",
        "\n",
        "def validate_image(filepath):\n",
        "    try:\n",
        "        with Image.open(filepath) as img:\n",
        "            img.verify()\n",
        "        print(f\"Image at {filepath} is valid.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Invalid image file at {filepath}: {e}\")\n",
        "        raise ValueError(f\"Invalid image file: {e}\")\n",
        "\n",
        "def resize_mask_if_needed(mask_path, input_path):\n",
        "    with Image.open(input_path) as input_image:\n",
        "        with Image.open(mask_path) as mask_image:\n",
        "            if input_image.size != mask_image.size:\n",
        "                print(f\"Resizing mask from {mask_image.size} to {input_image.size}\")\n",
        "                resized_mask = mask_image.resize(input_image.size)\n",
        "                resized_mask.save(mask_path)\n",
        "\n",
        "# ベクトル化対象のオブジェクトリスト\n",
        "TEXT_PROMPTS = [\n",
        "     'text','Name tag', 'License plate', 'Mail', 'Documents', 'QR codes',\n",
        "        'barcodes', 'Map', 'Digital screens', 'information board',\n",
        "        'signboard', 'poster', 'sign', 'logo', 'card', 'window', 'mirror',\n",
        "        'Famous landmark', 'cardboard', 'manhole', 'utility pole'\n",
        "]\n",
        "BOX_THRESHOLD = 0.3\n",
        "TEXT_THRESHOLD = 0.3\n",
        "\n",
        "# クラスタリング結果をJSONファイルから読み込む関数\n",
        "def load_sums_from_json(filepath):\n",
        "    with open(filepath, 'r') as json_file:\n",
        "        sums = json.load(json_file)\n",
        "    return sums\n",
        "\n",
        "# ベクトルデータをJSONファイルから読み込む関数\n",
        "def load_vectors_from_json(filepath):\n",
        "    with open(filepath, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "    return data\n",
        "\n",
        "# 新しい画像を分類する関数\n",
        "def classify_new_image(new_image_vector, sums_data, loaded_vectors, loaded_object_names, k=1):\n",
        "    cluster_centers = []\n",
        "    for cluster in sums_data:\n",
        "        indices = [loaded_object_names.index(obj_name) for obj_name in cluster]\n",
        "        cluster_vectors = np.array([loaded_vectors[obj_name] for obj_name in cluster])\n",
        "        cluster_center = np.mean(cluster_vectors, axis=0)\n",
        "        cluster_centers.append(cluster_center)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(cluster_centers, range(len(cluster_centers)))\n",
        "\n",
        "    new_image_label = knn.predict([new_image_vector])\n",
        "    return new_image_label[0]\n",
        "\n",
        "# 画像ベクトル化の処理（例としての関数）\n",
        "def process_image_vec(image_path):\n",
        "    detection_model = load_model(CONFIG_PATH, WEIGHTS_PATH)\n",
        "\n",
        "    image_source, image = load_image(image_path)\n",
        "    object_vector = np.zeros(len(TEXT_PROMPTS))\n",
        "\n",
        "    for i, text_prompt in enumerate(TEXT_PROMPTS):\n",
        "        boxes, logits, phrases = predict(\n",
        "            model=detection_model,\n",
        "            image=image,\n",
        "            caption=text_prompt,\n",
        "            box_threshold=BOX_THRESHOLD,\n",
        "            text_threshold=TEXT_THRESHOLD\n",
        "        )\n",
        "\n",
        "        if len(logits) > 0:\n",
        "            logits_np = np.array(logits)\n",
        "            object_vector[i] = np.sum(logits_np)\n",
        "    print(object_vector)\n",
        "    return object_vector.tolist()\n",
        "\n",
        "# APIのエンドポイント\n",
        "@app.post(\"/classify-image/\")\n",
        "async def classify_image(file: UploadFile = File(...)):\n",
        "    image_path = \"/tmp/temp_image.jpg\"\n",
        "\n",
        "    # アップロードされた画像を保存\n",
        "    with open(image_path, \"wb\") as buffer:\n",
        "        buffer.write(await file.read())\n",
        "\n",
        "    # 画像をベクトル化\n",
        "    new_image_vector = process_image_vec(image_path)\n",
        "\n",
        "    # JSONファイルからデータを読み込む\n",
        "    json_filepath = \"/content/drive/MyDrive/lastsum/output_vectors.json\"\n",
        "    loaded_data = load_vectors_from_json(json_filepath)\n",
        "    loaded_vectors = {obj_name: np.array(vector) for obj_name, vector in loaded_data.items()}\n",
        "    loaded_object_names = list(loaded_vectors.keys())\n",
        "\n",
        "    # 既存のクラスタリング結果を読み込む\n",
        "    sums_data = load_sums_from_json(\"/content/drive/MyDrive/lastsum/sums_data.json\")\n",
        "\n",
        "    # 新しい画像がどのクラスタに分類されるかを判定\n",
        "    new_image_cluster = classify_new_image(new_image_vector, sums_data, loaded_vectors, loaded_object_names)\n",
        "\n",
        "    return {\"danger\":dangerarray[int(new_image_cluster + 1)]}#バグったらここを＋にして\n",
        "\n",
        "\n",
        "@app.post(\"/create-mask-and-inpaint-simple-lama-special\")\n",
        "async def create_mask_and_inpaint_simple_lama(\n",
        "    image: UploadFile = File(...),\n",
        "    risk_level: int = Form(...),\n",
        "    x1: float = Form(...),\n",
        "    y1: float = Form(...),\n",
        "    x2: float = Form(...),\n",
        "    y2: float = Form(...),\n",
        "):\n",
        "    # Extract points from the form data\n",
        "    point1 = [x1, y1]\n",
        "    point2 = [x2, y2]\n",
        "\n",
        "    # Save the input image\n",
        "    input_path = save_image(image.file, \"input.jpg\")\n",
        "    print('1111',point1,point2)\n",
        "    # Create a mask image (using the new process_image function)\n",
        "    mask_path = special_process_image(risk_level, input_path, point1, point2,thresholds=thresholds)\n",
        "\n",
        "    # Define the output path for the inpainted image\n",
        "    output_path = \"/content/output_simple_lama.jpg\"\n",
        "\n",
        "    # Perform inpainting with SimpleLama\n",
        "    inpaint_image_with_mask1(input_path, mask_path, output_path, resize_factor=1)\n",
        "\n",
        "    # Return the resulting image as a response\n",
        "    return FileResponse(output_path, media_type=\"image/jpeg\", filename=\"output_simple_lama.jpg\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def resize_mask_to_match(image_path, mask_path):\n",
        "    # オリジナル画像とマスク画像を読み込む\n",
        "    original_image = Image.open(image_path)\n",
        "    mask_image = Image.open(mask_path)\n",
        "\n",
        "    # マスク画像をオリジナル画像のサイズにリサイズ\n",
        "    resized_mask = mask_image.resize(original_image.size)\n",
        "\n",
        "    # マスク画像を上書き保存\n",
        "    resized_mask.save(mask_path)\n",
        "\n",
        "@app.post(\"/create-mask-and-inpaint-sum\")\n",
        "async def create_mask_sum(image: UploadFile = File(...), risk_level: int = Form(...),\n",
        "    x1: float = Form(...),\n",
        "    y1: float = Form(...),\n",
        "    x2: float = Form(...),\n",
        "    y2: float = Form(...),):\n",
        "\n",
        "    input_path = save_image(image.file, \"/content/saved_images/input.jpg\")\n",
        "    default_x = 0.001\n",
        "    default_y = 0.001\n",
        "\n",
        "\n",
        "    point1 = [default_x if math.isnan(x1) else x1, default_y if math.isnan(y1) else y1]\n",
        "\n",
        "    point2 = [default_x if math.isnan(x2) else x2, default_y if math.isnan(y2) else y2]\n",
        "\n",
        "\n",
        "    print('1111',point1,point2)\n",
        "    # Create a mask image (using the new process_image function)\n",
        "    mask_path = special_process_image(risk_level, input_path, point1, point2,thresholds=thresholds)\n",
        "\n",
        "    # Define the output path for the inpainted image\n",
        "    output_path = \"/content/output_simple_lama.jpg\"\n",
        "\n",
        "    # Perform inpainting with SimpleLama\n",
        "    inpaint_image_with_mask1(input_path, mask_path, output_path, resize_factor=1)\n",
        "\n",
        "    # Return the resulting image as a response\n",
        "    return FileResponse(output_path, media_type=\"image/jpeg\", filename=\"output_simple_lama.jpg\")\n",
        "    '''\n",
        "    mask_path = \"/content/final_mask.jpg\"  # マスク画像の保存パス\n",
        "    output_path = SAVE_DIR / \"output_simple_lama.jpg\"\n",
        "\n",
        "    # special_process_imageの実行\n",
        "    mask_path = special_process_image(risk_level, input_path, point1, point2, thresholds=thresholds)\n",
        "\n",
        "    # special_process_imageの後にマスク画像とオリジナル画像のサイズを揃える\n",
        "    resize_mask_to_match(input_path, mask_path)\n",
        "\n",
        "    original_image_path = input_path\n",
        "    mask_image_path = mask_path\n",
        "    small_threshold = 10.0   # 「すごく小さい」の上限（5%）\n",
        "    medium_threshold = 25.0  # 「小さい」の上限（20%）\n",
        "    large_threshold = 50.0   # 「大きい」の上限（50%）\n",
        "    # マスク画像を作成\n",
        "\n",
        "    if analyze_mask_combined(original_image_path, mask_image_path, small_threshold, medium_threshold, large_threshold)['size'] == 1:#サイズ1\n",
        "      if analyze_mask_combined(original_image_path, mask_image_path, small_threshold, medium_threshold, large_threshold)['brightness']==1:\n",
        "        #めっちゃ小さくて明るいもの\n",
        "        output_path = SAVE_DIR / \"output_opencv.jpg\"\n",
        "        # OpenCVでインペイント\n",
        "        inpaint_image_with_mask(input_path, mask_path, output_path)\n",
        "      elif analyze_mask_combined(original_image_path, mask_image_path, small_threshold, medium_threshold, large_threshold)['brightness']==2:\n",
        "         #めっちゃ小さくて暗いもの\n",
        "        inpaint_image_with_mask1(input_path, mask_path, output_path, resize_factor=1)#SLI\n",
        "\n",
        "    elif analyze_mask_combined(original_image_path, mask_image_path, small_threshold, medium_threshold, large_threshold)['size'] == 2: #サイズ2\n",
        "      if analyze_mask_combined(original_image_path, mask_image_path, small_threshold, medium_threshold, large_threshold)['brightness']==1:\n",
        "        #小さくて明るいもの\n",
        "        input_path = save_image(image.file, \"input.jpg\")\n",
        "        save_as_input=0\n",
        "        copy_image_to_folder(input_path, save_as_input)\n",
        "        input_path=process_image(risk_level,input_path, thresholds=thresholds)\n",
        "        save_as_input=1#option\n",
        "        copy_image_to_folder(input_path, save_as_input)\n",
        "        os.chdir(f\"{HOME}/deepfillv2_colab\")#一度ディレクトリを変更\n",
        "        run_python_script('/content/deepfillv2_colab/inpaint.py')#プロセス用のコードを呼び出す\n",
        "        #!python inpaint.py\n",
        "        os.chdir(f\"{HOME}/GroundingDINO\") #元に戻す\n",
        "        output_image_path='/content/deepfillv2_colab/output/inpainted_img.png' #DeepFillv2\n",
        "\n",
        "\n",
        "      elif analyze_mask_combined(original_image_path, mask_image_path, small_threshold, medium_threshold, large_threshold)['brightness']==2:\n",
        "         #小さくて暗いもの\n",
        "        inpaint_image_with_mask1(input_path, mask_path, output_path, resize_factor=1)#SLI\n",
        "\n",
        "\n",
        "    elif analyze_mask_combined(original_image_path, mask_image_path, small_threshold, medium_threshold, large_threshold)['size'] == 3: #サイズ3\n",
        "      if analyze_mask_combined(original_image_path, mask_image_path, small_threshold, medium_threshold, large_threshold)['brightness']==1:\n",
        "        #おおきくて明るいもの\n",
        "        input_path = save_image(image.file, \"input.jpg\")\n",
        "        save_as_input=0\n",
        "        copy_image_to_folder(input_path, save_as_input)\n",
        "        input_path=process_image(risk_level,input_path, thresholds=thresholds)\n",
        "        save_as_input=1#option\n",
        "        copy_image_to_folder(input_path, save_as_input)\n",
        "        os.chdir(f\"{HOME}/deepfillv2_colab\")#一度ディレクトリを変更\n",
        "        run_python_script('/content/deepfillv2_colab/inpaint.py')#プロセス用のコードを呼び出す\n",
        "        #!python inpaint.py\n",
        "        os.chdir(f\"{HOME}/GroundingDINO\") #元に戻す\n",
        "        output_image_path='/content/deepfillv2_colab/output/inpainted_img.png' #DeepFillv2\n",
        "\n",
        "\n",
        "      elif analyze_mask_combined(original_image_path, mask_image_path, small_threshold, medium_threshold, large_threshold)['brightness']==2:\n",
        "         #おおきくて暗いもの\n",
        "        inpaint_image_with_mask1(input_path, mask_path, output_path, resize_factor=1)#SLI\n",
        "\n",
        "\n",
        "    elif analyze_mask_combined(original_image_path, mask_image_path, small_threshold, medium_threshold, large_threshold)['size'] == 4: #サイズ4\n",
        "      if analyze_mask_combined(original_image_path, mask_image_path, small_threshold, medium_threshold, large_threshold)['brightness']==1:\n",
        "        #すごくおおきくて明るいもの\n",
        "        inpaint_images([input_path], [mask_path], output_dir=SAVE_DIR)#diffusion\n",
        "\n",
        "      elif analyze_mask_combined(original_image_path, mask_image_path, small_threshold, medium_threshold, large_threshold)['brightness']==2:\n",
        "         #すごくおおきくて暗いもの\n",
        "        inpaint_image_with_mask1(input_path, mask_path, output_path, resize_factor=1)#SLI\n",
        "\n",
        "\n",
        "    return FileResponse(output_path)\n",
        "'''\n",
        "\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "async def read_root():\n",
        "    html_content = \"\"\"\n",
        " <!DOCTYPE html>\n",
        "<html lang=\"ja\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>画像処理アプリ</title>\n",
        "    <!-- Bootstrap CSS -->\n",
        "    <link href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
        "    <!-- jQuery UI CSS -->\n",
        "    <link rel=\"stylesheet\" href=\"https://code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css\">\n",
        "    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css\" rel=\"stylesheet\">\n",
        "    <style>\n",
        "        body {\n",
        "            background-color: #f0f0f5;\n",
        "            color: #333;\n",
        "            text-align: center;\n",
        "            padding: 40px 20px;\n",
        "        }\n",
        "        h1 {\n",
        "            color: #555;\n",
        "            margin-bottom: 30px;\n",
        "            font-weight: bold;\n",
        "        }\n",
        "        .image-preview, .processed-preview {\n",
        "            max-width: 100%;\n",
        "            height: auto;\n",
        "            border-radius: 10px;\n",
        "            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);\n",
        "            margin-top: 20px;\n",
        "        }\n",
        "        #result {\n",
        "            margin-top: 40px;\n",
        "            display: none;\n",
        "        }\n",
        "        .slider-container {\n",
        "            text-align: left;\n",
        "            margin-top: 20px;\n",
        "        }\n",
        "        .slider-label {\n",
        "            font-size: 1.2rem;\n",
        "            color: #333;\n",
        "        }\n",
        "        #slider {\n",
        "            margin-top: 10px;\n",
        "        }\n",
        "        .btn-primary {\n",
        "            background-color: #007bff;\n",
        "            border-color: #007bff;\n",
        "            font-size: 1.2rem;\n",
        "            padding: 10px 20px;\n",
        "            border-radius: 50px;\n",
        "        }\n",
        "        .btn-primary:hover {\n",
        "            background-color: #0056b3;\n",
        "            border-color: #004085;\n",
        "        }\n",
        "        .form-control, .custom-select {\n",
        "            border-radius: 20px;\n",
        "            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);\n",
        "        }\n",
        "        .form-control-file {\n",
        "            font-size: 1rem;\n",
        "        }\n",
        "        .form-group {\n",
        "            margin-bottom: 25px;\n",
        "        }\n",
        "        .btn-success {\n",
        "            padding: 10px 20px;\n",
        "            border-radius: 50px;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <h1><i class=\"fas fa-image\"></i> 画像処理アプリ - モザイクとインペイント</h1>\n",
        "\n",
        "        <div class=\"form-group\">\n",
        "            <input type=\"file\" id=\"uploadImage\" class=\"form-control-file\" accept=\"image/*\" onchange=\"previewImage()\">\n",
        "        </div>\n",
        "        <img id=\"uploadedImage\" class=\"image-preview\" src=\"#\" alt=\"アップロードされた画像\" style=\"display: none;\">\n",
        "\n",
        "        <div class=\"form-group mt-4\">\n",
        "            <label for=\"processingType\">処理方法を選択:</label>\n",
        "            <select id=\"processingType\" class=\"custom-select\">\n",
        "                <option value=\"opencv\">OpenCVインペイント</option>\n",
        "                <option value=\"simple_lama\">Simple Lamaインペイント</option>\n",
        "                <option value=\"stable_diffusion\">Stable Diffusionインペイント</option>\n",
        "            </select>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"slider-container\">\n",
        "            <label for=\"riskLevel\" class=\"slider-label\">リスクレベル (0-100): <span id=\"riskLevelLabel\">50</span></label>\n",
        "            <div id=\"slider\"></div>\n",
        "        </div>\n",
        "\n",
        "        <button class=\"btn btn-primary mt-4\" onclick=\"processImage()\">処理開始</button>\n",
        "\n",
        "        <div id=\"result\" class=\"mt-5\">\n",
        "            <h2>処理結果:</h2>\n",
        "            <img id=\"processedImage\" class=\"processed-preview\" src=\"\" alt=\"\">\n",
        "            <a id=\"downloadLink\" class=\"btn btn-success mt-3\" href=\"#\" download=\"processed_image.jpg\">処理された画像をダウンロード</a>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <!-- jQuery and Bootstrap JS -->\n",
        "    <script src=\"https://code.jquery.com/jquery-3.5.1.min.js\"></script>\n",
        "    <script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js\"></script>\n",
        "    <!-- jQuery UI -->\n",
        "    <script src=\"https://code.jquery.com/ui/1.12.1/jquery-ui.js\"></script>\n",
        "\n",
        "    <script>\n",
        "        $(function() {\n",
        "            // スライダーの設定\n",
        "            $(\"#slider\").slider({\n",
        "                range: \"min\",\n",
        "                value: 50, // 初期値\n",
        "                min: 0,\n",
        "                max: 100,\n",
        "                slide: function(event, ui) {\n",
        "                    $(\"#riskLevelLabel\").text(ui.value);\n",
        "                }\n",
        "            });\n",
        "        });\n",
        "\n",
        "        function previewImage() {\n",
        "            const fileInput = document.getElementById('uploadImage');\n",
        "            const uploadedImage = document.getElementById('uploadedImage');\n",
        "\n",
        "            if (fileInput.files && fileInput.files[0]) {\n",
        "                const reader = new FileReader();\n",
        "                reader.onload = function (e) {\n",
        "                    uploadedImage.src = e.target.result;\n",
        "                    uploadedImage.style.display = 'block';\n",
        "                };\n",
        "                reader.readAsDataURL(fileInput.files[0]);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        function processImage() {\n",
        "            const fileInput = document.getElementById('uploadImage');\n",
        "            const processingType = document.getElementById('processingType').value;\n",
        "            const riskLevel = $(\"#slider\").slider(\"value\");  // スライダーから値を取得\n",
        "            const resultDiv = document.getElementById('result');\n",
        "            const processedImage = document.getElementById('processedImage');\n",
        "            const downloadLink = document.getElementById('downloadLink');\n",
        "\n",
        "            if (fileInput.files.length === 0) {\n",
        "                alert(\"画像を選択してください。\");\n",
        "                return;\n",
        "            }\n",
        "\n",
        "            const file = fileInput.files[0];\n",
        "            const formData = new FormData();\n",
        "            formData.append('image', file);\n",
        "            formData.append('risk_level', riskLevel);  // リスクレベルを追加\n",
        "\n",
        "            let apiEndpoint;\n",
        "            if (processingType === \"opencv\") {\n",
        "                apiEndpoint = \"/create-mask-and-inpaint-opencv\";\n",
        "            } else if (processingType === \"simple_lama\") {\n",
        "                apiEndpoint = \"/create-mask-and-inpaint-simple-lama\";\n",
        "            } else if (processingType === \"stable_diffusion\") {\n",
        "                apiEndpoint = \"/create-mask-and-inpaint-stable-diffusion\";\n",
        "            }\n",
        "\n",
        "            const url = \"https://wired-kitten-adequately.ngrok-free.app\" + apiEndpoint;\n",
        "\n",
        "            fetch(url, {\n",
        "                method: 'POST',\n",
        "                body: formData\n",
        "            })\n",
        "            .then(response => {\n",
        "                if (!response.ok) {\n",
        "                    throw new Error(\"Network response was not ok\");\n",
        "                }\n",
        "                return response.blob();\n",
        "            })\n",
        "            .then(blob => {\n",
        "                const objectURL = URL.createObjectURL(blob);\n",
        "                processedImage.src = objectURL;\n",
        "                downloadLink.href = objectURL;\n",
        "                resultDiv.style.display = \"block\";\n",
        "            })\n",
        "            .catch(error => {\n",
        "                console.error(\"画像処理に失敗しました。\", error);\n",
        "                alert(\"画像処理に失敗しました。\");\n",
        "            });\n",
        "        }\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    return HTMLResponse(content=html_content)\n",
        "\n",
        "\n",
        "def run_fastapi():\n",
        "    if __name__ == \"__main__\":\n",
        "        nest_asyncio.apply()\n",
        "        uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "def run_ngrok():\n",
        "    os.system(\"ngrok http --domain=wired-kitten-adequately.ngrok-free.app 8000\")#固定URLを発行\n",
        "\n",
        "# スレッドを作成\n",
        "fastapi_thread = threading.Thread(target=run_fastapi)\n",
        "ngrok_thread = threading.Thread(target=run_ngrok)\n",
        "\n",
        "# 二つのスレッドを動かす\n",
        "fastapi_thread.start()\n",
        "ngrok_thread.start()\n",
        "fastapi_thread.join()\n",
        "ngrok_thread.join()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j8vGNXhpEWF",
        "outputId": "76365646-d07a-43ad-f417-33488f969627"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.system(\"ngrok http --domain=wired-kitten-adequately.ngrok-free.app 8000\")#固定URLを発行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHSlB4ZN82ms",
        "outputId": "9cc5160d-3263-489b-d076-efc129f5967d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5633\n"
          ]
        }
      ],
      "source": [
        "\n",
        "DIR = '/content/drive/MyDrive/学習データ'\n",
        "\n",
        "print(sum(os.path.isfile(os.path.join(DIR, name)) for name in os.listdir(DIR)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKHQvElQ7smI",
        "outputId": "75dc15dc-5ee0-4f87-be89-31bdc7bb6470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root          67  0.0  0.0      0     0 ?        Z    Oct10   0:06 [python3] <defunct>\n",
            "root          68  0.0  0.3  67728 52272 ?        S    Oct10   0:00 python3 /usr/local/bin/colab-file\n",
            "root         117  0.0  1.3 575896 177940 ?       Sl   Oct10   0:08 /usr/bin/python3 /usr/local/bin/j\n",
            "root        1060  0.0  0.0  18024  9556 ?        S    Oct10   0:00 python3 /opt/google/drive/drive-f\n",
            "root        8542  0.0  0.1 538260 13624 ?        Sl   Oct10   0:00 /usr/bin/python3 /usr/local/lib/p\n",
            "root       72952 61.7 18.2 11576148 2423456 ?    Ssl  03:51   0:11 /usr/bin/python3 -m colab_kernel_\n",
            "root       72973  0.4  0.1 539284 13532 ?        Sl   03:51   0:00 /usr/bin/python3 /usr/local/lib/p\n",
            "root       73118  0.0  0.0   6484  2280 ?        S    03:51   0:00 grep python\n"
          ]
        }
      ],
      "source": [
        "!ps aux | grep 'python'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLhnSM1eRpII"
      },
      "source": [
        "下記は、最初のクラスタリングに必要なコード。今は何も手を付けなくていい\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4AyVOL471vn",
        "outputId": "faa72e13-e1af-417d-b106-ecf2979c5802"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: kill: (15108) - No such process\n"
          ]
        }
      ],
      "source": [
        "!kill 15108"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "gTdNyBLEVePg",
        "outputId": "93e174dd-97cf-4fad-e80c-5e41f16fdb0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.49639297 0.53853089 0.68880463 0.99608672 1.06955934 0.96696973\n",
            " 1.14053011 0.56766939 0.78465068 1.0010705  0.8178345  0.440348\n",
            " 0.98328477 1.37204599 0.92802715 0.5291034  0.76120877 1.04426539\n",
            " 0.98724872 1.01390123 0.87089682]\n",
            "[0.70044339 0.30440947 0.68930662 0.40916607 0.55707008 0.\n",
            " 0.         0.71923631 0.         0.46680602 0.31489816 0.\n",
            " 0.         0.69141614 0.35253656 0.32804587 0.68376303 0.42671692\n",
            " 0.64998502 0.         0.76780516]\n",
            "[1.22471309 0.6068123  0.64947981 1.25715756 0.8556605  0.4840408\n",
            " 0.65523833 0.96553457 0.         0.70873404 0.66456181 1.04295528\n",
            " 0.75752252 1.25871253 0.86201441 0.82422018 0.8417719  0.63659412\n",
            " 0.8904956  0.73832071 0.92354989]\n",
            "[0.47478062 0.         1.07758141 0.34041968 0.43952766 0.\n",
            " 0.57516688 0.         0.         1.18100524 0.40632349 0.39842549\n",
            " 0.         0.84396023 0.31448552 0.76259035 0.74724555 0.36352268\n",
            " 0.41878954 0.34494674 0.38678753]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bc4a1f0e3460>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# フォルダ内の画像を処理してJSONに保存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mprocess_images_in_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_json_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-bc4a1f0e3460>\u001b[0m in \u001b[0;36mprocess_images_in_folder\u001b[0;34m(folder_path, output_json_path)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.jpeg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mobject_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_image_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-bc4a1f0e3460>\u001b[0m in \u001b[0;36mprocess_image_vec\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_prompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT_PROMPTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         boxes, logits, phrases = predict(\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetection_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GroundingDINO/groundingdino/util/inference.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, image, caption, box_threshold, text_threshold, device, remove_combined)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mprediction_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# prediction_logits.shape = (nq, 256)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GroundingDINO/groundingdino/models/GroundingDINO/groundingdino.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, samples, targets, **kw)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0minput_query_bbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_query_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdn_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         hs, reference, hs_enc, ref_enc, init_box_proposal = self.transformer(\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0msrcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_query_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_query_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GroundingDINO/groundingdino/models/GroundingDINO/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, srcs, masks, refpoint_embed, pos_embeds, tgt, attn_mask, text_dict)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;31m# Begin Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m#########################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         memory, memory_text = self.encoder(\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0msrc_flatten\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlvl_pos_embed_flatten\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GroundingDINO/groundingdino/models/GroundingDINO/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, pos, spatial_shapes, level_start_index, valid_ratios, key_padding_mask, memory_text, text_attention_mask, pos_text, text_self_attention_masks, position_ids)\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;31m# main process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_transformer_ckpt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 output = checkpoint.checkpoint(\n\u001b[0m\u001b[1;32m    577\u001b[0m                     \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                 \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;34m\"use_reentrant=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             )\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         gen = _checkpoint_without_reentrant_generator(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GroundingDINO/groundingdino/models/GroundingDINO/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, pos, reference_points, spatial_shapes, level_start_index, key_padding_mask)\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0;31m# self attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;31m# import ipdb; ipdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         src2 = self.self_attn(\n\u001b[0m\u001b[1;32m    786\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_pos_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0mreference_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreference_points\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GroundingDINO/groundingdino/models/GroundingDINO/ms_deform_attn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, query_pos, key_padding_mask, reference_points, spatial_shapes, level_start_index, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mspatial_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mspatial_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#統合用コード,vector作成\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# ベクトル化対象のオブジェクトリスト\n",
        "TEXT_PROMPTS = [\n",
        "    'text', 'mirror', 'window', 'cardboard', 'poster', 'sign', 'manhole',\n",
        "    'logo', 'signboard', 'utility pole','Name tag','License plate','card','Mail or envelope',\n",
        "    'Digital screens','QR codes','barcodes','documents','information board','Map',\n",
        "    'Famous landmark'\n",
        "]\n",
        "BOX_THRESHOLD = 0.3\n",
        "TEXT_THRESHOLD = 0.3\n",
        "\n",
        "def process_image_vec(image_path):\n",
        "    image_source, image = load_image(image_path)\n",
        "    object_vector = np.zeros(len(TEXT_PROMPTS))\n",
        "\n",
        "    for i, text_prompt in enumerate(TEXT_PROMPTS):\n",
        "        boxes, logits, phrases = predict(\n",
        "            model=detection_model,\n",
        "            image=image,\n",
        "            caption=text_prompt,\n",
        "            box_threshold=BOX_THRESHOLD,\n",
        "            text_threshold=TEXT_THRESHOLD\n",
        "        )\n",
        "\n",
        "        if len(logits) > 0:\n",
        "            logits_np = np.array(logits)\n",
        "            object_vector[i] = np.sum(logits_np)\n",
        "    print(object_vector)\n",
        "    return object_vector.tolist()\n",
        "\n",
        "def process_images_in_folder(folder_path, output_json_path):\n",
        "    results = {}\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            object_vector = process_image_vec(image_path)\n",
        "            results[filename] = object_vector\n",
        "\n",
        "    with open(output_json_path, 'w') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "# 画像が格納されているフォルダのパスと出力JSONファイルのパス\n",
        "folder_path = \"/content/drive/MyDrive/lastsum\"\n",
        "output_json_path = \"/content/drive/MyDrive/lastsum/output_vectors.json\"\n",
        "\n",
        "# フォルダ内の画像を処理してJSONに保存\n",
        "process_images_in_folder(folder_path, output_json_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HFSKxgfVx7j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MiniBatchKMeans, Birch, SpectralClustering, MeanShift, OPTICS\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "def load_from_json(filepath):\n",
        "    with open(filepath, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "    return list(data.values()), list(data.keys())\n",
        "\n",
        "# JSONファイルのパスを入力\n",
        "json_filepath = \"/content/drive/MyDrive/lastsum/output_vectors.json\"  # JSONデータが保存されたファイルパス\n",
        "\n",
        "# JSONファイルからデータを読み込む\n",
        "loaded_vectors, loaded_object_names = load_from_json(json_filepath)\n",
        "X = np.array(loaded_vectors)\n",
        "num = len(loaded_vectors)\n",
        "\n",
        "# クラスタリング手法のリスト\n",
        "clustering_methods = [\n",
        "    ('KMeans', KMeans),\n",
        "    ('MiniBatchKMeans', MiniBatchKMeans),\n",
        "    ('AgglomerativeClustering', AgglomerativeClustering),\n",
        "    ('DBSCAN', DBSCAN),\n",
        "    ('Birch', Birch),\n",
        "    ('SpectralClustering', SpectralClustering),\n",
        "    ('MeanShift', MeanShift),\n",
        "    ('OPTICS', OPTICS)\n",
        "]\n",
        "\n",
        "# クラスタリング手法を選択\n",
        "print(\"Select a clustering method:\")\n",
        "for i, (name, method) in enumerate(clustering_methods):\n",
        "    print(f\"{i}: {name}\")\n",
        "method_index = int(input(\"Enter the number of the clustering method: \"))\n",
        "method_name, ClusteringMethod = clustering_methods[method_index]\n",
        "\n",
        "# クラスタ数を選択する範囲\n",
        "k_range = range(8, min(100, num))\n",
        "# シルエットスコアの変化量を使って最適なクラスタ数を見つける\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    if method_name in ['DBSCAN', 'MeanShift', 'OPTICS']:\n",
        "        clustering = ClusteringMethod()\n",
        "        labels = clustering.fit_predict(X)\n",
        "        k = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    else:\n",
        "        clustering = ClusteringMethod(n_clusters=k)\n",
        "        labels = clustering.fit_predict(X)\n",
        "\n",
        "    if len(set(labels)) > 1:\n",
        "        silhouette_scores.append(silhouette_score(X, labels))\n",
        "\n",
        "# シルエットスコアの変化量を計算\n",
        "silhouette_deltas = np.diff(silhouette_scores)\n",
        "\n",
        "# 変化量が最大のインデックスを取得\n",
        "max_delta_index = np.argmax(silhouette_deltas)\n",
        "\n",
        "# 最適なクラスタ数\n",
        "optimal_k = k_range[max_delta_index + 1]  # np.diffの結果は1つ少ないため、インデックスを調整\n",
        "\n",
        "print(f'Optimal number of clusters based on silhouette score change: {optimal_k}')\n",
        "\n",
        "# シルエットスコアと変化量のプロット\n",
        "plt.figure()\n",
        "plt.plot(k_range[:len(silhouette_scores)], silhouette_scores, marker='o', label=\"Silhouette Score\")\n",
        "plt.plot(k_range[1:len(silhouette_scores)], silhouette_deltas, marker='x', label=\"Silhouette Score Change\", color='red')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Score / Change')\n",
        "plt.title(f'Silhouette Scores and Changes for {method_name}')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 最適なクラスタ数でクラスタリング\n",
        "if method_name in ['DBSCAN', 'MeanShift', 'OPTICS']:\n",
        "    clustering = ClusteringMethod()\n",
        "else:\n",
        "    clustering = ClusteringMethod(n_clusters=optimal_k)\n",
        "labels = clustering.fit_predict(X)\n",
        "\n",
        "# PCAで2次元に圧縮\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# クラスタの名前を定義\n",
        "cluster_names = [f'Cluster {i+1}' for i in range(len(set(labels)) - (1 if -1 in labels else 0))]\n",
        "\n",
        "# 2次元プロット\n",
        "plt.figure()\n",
        "for i in range(len(cluster_names)):\n",
        "    plt.scatter(X_pca[labels == i, 0], X_pca[labels == i, 1], label=cluster_names[i])\n",
        "plt.title(f'{method_name} Clustering (2D projection)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# クラスタ結果を保存\n",
        "sums = []\n",
        "for i in range(len(cluster_names)):\n",
        "    temp = [loaded_object_names[l] for l in range(len(labels)) if labels[l] == i]\n",
        "    sums.append(temp)\n",
        "print(sums)\n",
        "\n",
        "# sumsをJSONファイルに保存\n",
        "def save_sums_to_json(sums, filename='sums_data.json'):\n",
        "    with open(filename, 'w') as json_file:\n",
        "        json.dump(sums, json_file, indent=4)\n",
        "\n",
        "# JSONファイルの保存\n",
        "output_filepath = \"/content/drive/MyDrive/lastsum/sums_data.json\"  # 出力するJSONファイルのパス\n",
        "save_sums_to_json(sums, output_filepath)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m80jSBXn6VeX"
      },
      "source": [
        "この下は各クラスタの様子を調べるためのコード\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEjaL7zJWCpy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# JSONファイルからsumsを読み込む関数\n",
        "def load_sums_from_json(filepath):\n",
        "    with open(filepath, 'r') as json_file:\n",
        "        sums = json.load(json_file)\n",
        "    return sums\n",
        "\n",
        "# JSONファイルのパス\n",
        "json_filepath = \"/content/drive/MyDrive/lastsum/sums_data.json\"\n",
        "\n",
        "# sumsを読み込む\n",
        "sums = load_sums_from_json(json_filepath)\n",
        "print(sums)\n",
        "\n",
        "# クラスタ番号を入力\n",
        "num1 = int(input('クラスタ番号は?')) - 1\n",
        "print(f'{sums[num1]}の写真がクラスタ{num1+1}に属しています')\n",
        "\n",
        "# 画像のディレクトリパス\n",
        "images_dir = \"/content/drive/MyDrive/lastsum\"  # 画像が保存されているディレクトリのパスを指定\n",
        "\n",
        "# クラスタに含まれる画像を一枚ずつ表示する関数\n",
        "def display_images_one_by_one(cluster_images):\n",
        "    for image_filename in cluster_images:\n",
        "        # ファイル名から画像番号を抽出\n",
        "        image_name = image_filename.split('.')[0]\n",
        "        image_path = os.path.join(images_dir, f'{image_name}.jpg')\n",
        "\n",
        "        # 画像を読み込み\n",
        "        img = Image.open(image_path)\n",
        "\n",
        "        # 画像を大きく表示\n",
        "        plt.figure(figsize=(10, 10))  # 画像の表示サイズを指定\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Image {image_name}')\n",
        "        plt.show()\n",
        "\n",
        "# クラスタの画像を一枚ずつ表示\n",
        "display_images_one_by_one(sums[num1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anGbvqTwktuw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random  # ランダムに画像を選択するために必要\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# JSONファイルからsumsを読み込む関数\n",
        "def load_sums_from_json(filepath):\n",
        "    with open(filepath, 'r') as json_file:\n",
        "        sums = json.load(json_file)\n",
        "    return sums\n",
        "\n",
        "# JSONファイルのパス\n",
        "json_filepath = \"/content/drive/MyDrive/lastsum/sums_data.json\"\n",
        "\n",
        "# sumsを読み込む\n",
        "sums = load_sums_from_json(json_filepath)\n",
        "print(sums)\n",
        "\n",
        "# クラスタ番号を入力\n",
        "num1 = int(input('クラスタ番号は?')) - 1\n",
        "print(f'{sums[num1]}の写真がクラスタ{num1+1}に属しています')\n",
        "\n",
        "# 画像のディレクトリパス\n",
        "images_dir = \"/content/drive/MyDrive/lastsum\"  # 画像が保存されているディレクトリのパスを指定\n",
        "\n",
        "# クラスタに含まれる画像からランダムに3枚表示する関数\n",
        "def display_random_images(cluster_images, num_images=10):\n",
        "    # クラスタ内の画像数がnum_images未満の場合、全ての画像を表示\n",
        "    if len(cluster_images) < num_images:\n",
        "        selected_images = cluster_images\n",
        "    else:\n",
        "        selected_images = random.sample(cluster_images, num_images)  # ランダムに3枚選択\n",
        "\n",
        "    for image_filename in selected_images:\n",
        "        # ファイル名から画像番号を抽出\n",
        "        image_name = image_filename.split('.')[0]\n",
        "        image_path = os.path.join(images_dir, f'{image_name}.jpg')\n",
        "\n",
        "        # 画像を読み込み\n",
        "        img = Image.open(image_path)\n",
        "\n",
        "        # 画像を大きく表示\n",
        "        plt.figure(figsize=(10, 10))  # 画像の表示サイズを指定\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Image {image_name}')\n",
        "        plt.show()\n",
        "\n",
        "# クラスタの画像からランダムに3枚表示\n",
        "display_random_images(sums[num1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_sebtgJVwVT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TXirZ-jM3lF"
      },
      "outputs": [],
      "source": [
        "!pip freeze -> '/content/requirements.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViNkGPGl-K-t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}